{"cells":[{"cell_type":"markdown","metadata":{"id":"O_hMTz71czWs"},"source":["<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"]},{"cell_type":"markdown","metadata":{"id":"1c3XC6EzczWy"},"source":["# 3. NVIDIA FasterTransformer를 사용한 GPT-J 6b 추론\n","\n","\n","이번 실습에서는 NVIDIA FasterTransformer 라이브러리를 살펴보고, 이 라이브러리의 작동 방식 및 모델에 적용되는 최적화 기법에 대해 설명하겠습니다. \n","이후, NVIDIA FasterTransformers를 사용하여 6b의 매개 변수를 가진 큰 GPT-J 모델의 추론을 실행하는 방법에 대한 기본 예제를 실행해 봅니다.\n","그리고, 텐서 병렬(TP) 및/또는 파이프라인 병렬(PP)과 같은 다양한 추론 모드에 대해 살펴보겠습니다. 또한 HuggingFace와 FasterTransformer TP/PP 추론 접근법 간의 지연 시간을 비교할 것입니다.\n","\n","## 목표\n","\n","이번 노트북의 목표는 다음과 같습니다:\n","* NVIDIA FasterTransformer 라이브러리가 무엇인지 알아봅니다\n","* 이 라이브러리를 구축하는 방법에 대해 알아봅니다\n","* FasterTransformer를 사용하여 추론을 위해 Transformer 모델의 체크포인트를 준비하는 방법에 대해 알아봅니다\n","* 하나의 GPU에서 모델의 추론을 실행합니다\n","* 텐서 병렬화 기법을 사용하여 여러 GPU에서 모델 추론을 실행합니다\n","* 지연 시간을 확인하기 위해 모든 파이프라인을 프로파일링합니다.\n","* 추론 속도를 측정하고 프로파일링 결과를 분석합니다.\n","\n","\n","**[3.1 NVIDIA FasterTransformer](#3.1)<br>**\n","**[3.2 NVIDIA FasterTransformer 전체적인 추론 파이프라인](#3.2)<br>**\n","**[3.3 NVIDIA FasterTransformer 라이브러리 다운로드 및 구축하기](#3.3)<br>**\n","**[3.4 GPT-J 체크포인트 다운로드 및 준비하기](#3.4)<br>**\n","**[3.5 추론을 위해 FT-친화적인 포맷으로 가중치 변환하기](#3.5)<br>**\n","**[3.6 C++ 바인딩 활용 GPT-J 추론](#3.6)<br>**\n","&nbsp;&nbsp;&nbsp;&nbsp;[3.6.1 1-GPU 추론 ](#3.4.1)<br>\n","&nbsp;&nbsp;&nbsp;&nbsp;[3.6.2 2-GPU 추론 ](#3.4.2)<br>"]},{"cell_type":"markdown","metadata":{"id":"5kTl48LLczWz"},"source":["# 3.1 NVIDIA FasterTransformer"]},{"cell_type":"markdown","metadata":{"id":"Nlw-40ByczW0"},"source":["## FasterTransformer 개요\n","\n","[NVIDIA FasterTransformer (FT)](https://github.com/NVIDIA/FasterTransformer) 는 분산 방식으로 거대 모델의 추론에 중점을 둔 트랜스포머 기반 신경망 추론을 위한 가속 엔진을 구현하는 라이브러리입니다. \n","FasterTransformer에는 인코더 및 디코더 부품이 포함된 고도로 최적화된 버전의 Transformer 블록이 구현되어 있습니다. 이 블록을 사용하여 T5와 같은 전체 인코더-디코더 아키텍처를 가진 파이프라인으로 추론을 실행할 수 있을 뿐만 아니라 BERT-Like(인코더 전용) 또는 GPT-3 Like(디코더 전용)한 모델의 추론을 별도로 수행할 수 있습니다. C++/CUDA로 작성되었으며 GPU에서 가장 빠른 계산 파이프라인을 구축할 수 있는 cuBLAS, cuBLASlt​, cuSPARSELt 라이브러리에 의존합니다.\n","\n","## GPT 모델 소개\n","GPT와 GPT-J는 인코더 모듈이 없고 멀티 헤드 어텐션을 교차하며 액티베이션으로 GeLU를 사용하는 디코딩 모델의 변형입니다. 2020년에 OpenAI는 [자체 논문](https://arxiv.org/abs/2005.14165)에서 거대 모델과 많은 훈련 데이터를 사용하면 GPT 모델의 용량을 크게 향상시킬 수 있음을 보여주었습니다. 예를 들어, 가장 큰 모델인 GPT-3에는 1750억 개의 매개 변수가 있으며, 이는 반 정밀도로 약 350GB가 소요됩니다. 그러나 이러한 모델을 단일 GPU에 넣는 것은 불가능합니다. 따라서 멀티-GPU, 심지어 멀티-노드도 필요합니다. 모델 크기로 인한 지연 시간 및 메모리의 병목 현상을 해결하기 위해 FasterTransformer는 여러 프레임워크에서 높은 효율성, 최적화된 메모리 사용 및 모델 병렬화를 제공하는 커널을 제공합니다. \n","\n","이 문서에서는 FasterTransformer가 GPT-J 모델에 제공하는 기능과 워크플로우 및 최적화를 설명합니다. 또한 사용자가 FasterTransformer에서 GPT-J 모델을 실행할 수 있도록 지원하는 가이드를 제공합니다. \n","\n","### FasterTransformer - Supported features:\n","\n","* 체크포인트 컨버터\n","  * Huggingface\n","  * Megatron\n","  * Nemo Megatron\n","  * TensorFlow\n","* 데이터 타입\n","  * FP32\n","  * FP16\n","  * INT8 weight only PTQ for bs 1 and 2\n","* 기능\n","  * 멀티-GPU 멀티-노드 추론\n","  * 다이내믹 랜덤 시드 (Dynamic random seed)\n","  * 토큰 중지 (Stop tokens)\n","  * 빔 검색 및 샘플링 (Beam search and sampling)\n","  * FP32, FP16와 INT8 추론\n","* 프레임워크\n","  * TensorFlow\n","  * PyTorch\n","  * C++\n","  * Triton 백엔드"]},{"cell_type":"markdown","metadata":{"id":"fw4DvPGyczW1"},"source":["### 텐서-병렬화 및 파이프라인-병렬화\n","\n","FasterTransformer는 MPI 및 NVIDIA NCCL을 사용하여 노드 내/노드 간 통신을 지원합니다. 이 소프트웨어 스택을 사용하면 누구나 여러 GPU에서 텐서-병렬 모드로 거대 트랜스포머를 실행하여 계산 지연 시간을 줄일 수 있습니다. 동시에, 텐서 병렬화(TP)와 파이프라인 병렬화(PP)가 함께 결합되어 멀티 GPU 및 멀티 노드 환경에서 수십억 및 수조 개의 매개 변수(테라바이트의 가중치에 해당하는)를 가진 거대한 트랜스포머 모델을 실행할 수 있습니다. \n","\n","- DataParallel (DP) - 동일한 설정이 여러 번 복제되며, 각 설정은 데이터의 슬라이스를 제공합니다. 처리는 병렬로 수행되며 모든 설정은 각 훈련 단계가 끝날 때 동기화됩니다.\n","- TensorParallel (TP) - 각 텐서는 여러 개의 청크로 분할되므로 전체 텐서가 단일 GPU에 상주하는 대신 텐서의 각 샤드가 지정된 GPU에 상주합니다. 처리하는 동안 각 샤드는 서로 다른 GPU에서 별도로 병렬로 처리되고 결과는 단계가 끝날 때 동기화됩니다. 이것은 수평 레벨에서 분리가 일어나기 때문에 수평 평행(horizontal parallelism)라고 부르기도 합니다.\n","- Pipeline Parallel (PP) - 모델은 여러 GPU에 걸쳐 수직(레이어 레벨)으로 분할되므로 모델의 한 개 또는 여러 계층만 단일 GPU에 배치됩니다. 각 GPU는 파이프라인의 서로 다른 단계에서 병렬 처리되며 배치의 작은 청크에서 작동합니다.\n","\n","<img src=\"./images/image3.png\" style=\"width: 1000px;\">"]},{"cell_type":"markdown","metadata":{"id":"0ZsQSO6aczW2"},"source":["### FasterTransformer 라이브러리의 추론 최적화\n","\n","FT를 사용하면 딥 러닝 훈련을 위한 일반적인 프레임워크에 비해 트랜스포머 기반 NN에 대한 대기 시간이 짧고 처리량이 높아 더 빠른 추론 파이프라인을 얻을 수 있습니다. 다음은 FT가 GPT-3 메가트론 모델에 대해 가장 빠른 추론을 할 수 있도록 하는 몇 가지 최적화 기술을 소개합니다:\n","1. <b>Layer Fusion</b></br>\n","사전 처리 단계에 들어가 하나의 커널로 계산될 수 있도록 여러 층의 NN을 하나로 결합하는 일련의 기술입니다. 이 기술은 데이터 전송을 줄이고 연산적인 밀도를 높여서 추론 단계에서 계산을 가속화합니다. 예를 들어 멀티 헤드 어텐션 블록의 모든 작업을 하나의 커널로 결합할 수 있습니다.\n","2. <b>Keys/Values caching - 자기 회귀 모델에 대한 추론 최적화</b></br>\n","FT는 이전 키와 밸류 값을 재계산하지 않도록 각 단계에서 저장할 버퍼를 할당합니다. 추가 메모리 사용량이 필요하지만 FT는 재계산, 각 단계에서 버퍼 할당 및 연결 비용을 절약할 수 있습니다.<br/>\n","<div style=\"text-align:center\">\n","<img src=\"./images/KV_caching v2.PNG\" style=\"width: 60%;position:relative;\"><br/>\n","<em>Keys/Values caching</em>\n","</div>\n","<br/><br/>\n","3. <b> 메모리 최적화 </b> </br>\n","BERT와 같은 기존 모델과 달리 거대 트랜스포머 모델은 최대 수조 개의 매개 변수를 가집니다. 예를 들어 GPT-3 175b는 모델을 반 정밀도로 저장해도 350GB를 사용합니다. 따라서 우리는 다른 부분의 메모리 사용량을 줄여야 합니다. FasterTransformer에서는 서로 다른 디코더 계층의 메모리 버퍼를 재사용합니다. GPT-3의 레이어 수가 96개이기 때문에, 우리는 1/96 메모리만 있으면 됩니다.\n","4. <b>노드 내/노드 간 통신을 지원하고 모델 병렬화를 지원하는 MPI 및 NCCL 활용</b></br>\n","GPT 모델을 사용 시, FasterTransormer는 텐서와 파이프라인 병렬화를 모두 제공합니다. 텐서 병렬화를 위해, FasterTransformer는 Megatron의 아이디어를 따릅니다. 셀프 어텐션블록과 피드 포워드 네트워크 블록에서 FT는 첫 번째 행렬 곱의 가중치를 행으로 분할하고 두 번째 행렬 곱의 가중치를 열로 분할합니다. FT는 최적화를 통해 각 트랜스포머 블록에 대해 감소 연산을 2배까지 줄일 수 있습니다. 파이프라인 병렬화를 위해서FasterTransformer는 전체 요청 배치를 여러 개의 마이크로 배치로 분할하고 통신 버블들을 숨깁니다. FasterTransformer는 상황에 따라 마이크로 배치 크기를 자동으로 조정합니다. 사용자는 gpt_config.ini 파일을 수정하여 모델 병렬화를 조정할 수 있습니다. 텐서 병렬은 더 많은 NCCL 통신이 필요하기 때문에 텐서 병렬 인트라노드와 파이프라인 병렬 인터노드를 사용하는 것을 추천드립니다.\n","5. <b>MatMul 커널 오토 튜닝 (GEMM 자동 조정) </b> </br>\n","행렬 곱은 트랜스포머 기반 신경망에서 가장 중요하고 무거운 연산입니다. FT는 CuBLAS 및 CuTLASS 라이브러리의 기능을 사용하여 이러한 유형의 연산을 실행합니다. MatMul 연산은 \"하드웨어\" 수준에서 서로 다른 로우레벨 알고리즘을 사용하여 수십 가지 다른 방식으로 실행될 수 있다는 것을 아는 것이 중요합니다. GemmBatchedEx 함수는 MatMul 연산을 구현하고  “cublasGemmAlgo_t” 를 입력 매개 변수로 사용합니다. 이 매개 변수를 사용하여, 우리는 작동을 위해 다른 로우 레벨 알고리즘을 선택할 수 있습니다. FasterTransformer 라이브러리는 이 매개 변수를 사용하여 모든 로우 레벨 알고리즘에 대한 실시간 벤치마크를 수행하고 모델의 매개 변수(어텐션 레이어,크기, 어텐션 헤드 수, 히든 레이어 크기 등)와 입력 데이터에 가장 적합한 매개 변수를 선택합니다. 또한 FT는 __expf(), __shfl_xor_sync() 와 같은 일부 네트워크 부분에 하드웨어 가속 로우 레벨 함수를 사용합니다.\n","<div style=\"text-align:center\">\n","<img src=\"./images/kernel autotuning.png\" style=\"width: 20%;position:relative;\"><br/>\n","<em>Kernel autotuning</em>\n","</div>\n","<br/><br/>\n","6. <b>더 낮은 정밀도로 추론</b></br>\n","FP16, FP32, POC에서 GPT-3에 대한 포스트-트레이닝 가중치 전용 INT8 양자화\n","FT에는 fp16 및 int8 의 낮은 정밀도 입력 데이터를 사용하여 추론을 지원하는 커널이 있습니다. 이 두 체제 모두 데이터 전송이 적고 메모리가 필요하기 때문에 가속을 허용합니다. 동시에 int8 및 fp16 연산은 특수 하드웨어인 Tensor Core(Volta부터 시작하는 모든 GPU 아키텍처용)에서 실행될 수 있습니다. \n","7. <b>기타:</b></br>\n","    - 빠른 C++ BeamSearch 구현\n","    - TensorParallelism 8 모드에 최적화된 올-리듀스. (모델의 가중치가 8개의 GPU로 분할)"]},{"cell_type":"markdown","metadata":{"id":"eMpPiD6IczW4"},"source":["# 3.2 NVIDIA FasterTransformer를 통한 전반적인 추론 파이프라인"]},{"cell_type":"markdown","metadata":{"id":"1LVed0NhczW4"},"source":["<img src=\"./images/FT_pipeline.PNG\">"]},{"cell_type":"markdown","metadata":{"id":"npPVN1A2czW5"},"source":["## 3.3 Download and Build NVIDIA FasterTransformer library"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZWcLqZ4bczW6","executionInfo":{"status":"ok","timestamp":1661323753181,"user_tz":-540,"elapsed":7096,"user":{"displayName":"Solee Moon KR","userId":"15448835692925759007"}},"outputId":"44d2ae52-10d1-4745-bba0-de3618eb7ecf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'FasterTransformer'...\n","remote: Enumerating objects: 3838, done.\u001b[K\n","remote: Counting objects: 100% (3408/3408), done.\u001b[K\n","remote: Compressing objects: 100% (2003/2003), done.\u001b[K\n","remote: Total 3838 (delta 1828), reused 2401 (delta 1373), pack-reused 430\u001b[K\n","Receiving objects: 100% (3838/3838), 20.18 MiB | 16.56 MiB/s, done.\n","Resolving deltas: 100% (1926/1926), done.\n","Checking out files: 100% (1046/1046), done.\n","/content/FasterTransformer\n","Note: checking out '6b3fd4392831f972d48127e881a048567dd92811'.\n","\n","You are in 'detached HEAD' state. You can look around, make experimental\n","changes and commit them, and you can discard any commits you make in this\n","state without impacting any branches by performing another checkout.\n","\n","If you want to create a new branch to retain commits you create, you may\n","do so (now or later) by using -b with the checkout command again. Example:\n","\n","  git checkout -b <new-branch-name>\n","\n","HEAD is now at 6b3fd43 Update gpt_guide.md\n"]}],"source":["!git clone https://github.com/NVIDIA/FasterTransformer.git\n","%cd FasterTransformer\n","!git checkout 6b3fd4392831f972d48127e881a048567dd92811"]},{"cell_type":"markdown","metadata":{"id":"k8EpO8DBczW8"},"source":["### 라이브러리 구축하기"]},{"cell_type":"markdown","metadata":{"id":"ryFbo9kdczW8"},"source":["#### Install dependencies"]},{"cell_type":"markdown","metadata":{"id":"w5kgXmhrczW9"},"source":["We should install additional libraries like `cmake` for build automation, and we also should install compression libraries \"zstd\" and \"libz-dev\" that will allow decompressing weights of the GPT-J 6b model. </br>\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OK9QwTrAczW-","executionInfo":{"status":"ok","timestamp":1661323771878,"user_tz":-540,"elapsed":18701,"user":{"displayName":"Solee Moon KR","userId":"15448835692925759007"}},"outputId":"bba7c4f0-7ded-48e3-8493-757c186ee62b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","\u001b[33m\r0% [Waiting for headers] [1 InRelease 14.2 kB/88.7 kB 16%] [Connected to cloud.\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","\u001b[33m\r0% [Waiting for headers] [1 InRelease 14.2 kB/88.7 kB 16%] [2 InRelease 0 B/3,6\u001b[0m\u001b[33m\r0% [Waiting for headers] [1 InRelease 43.1 kB/88.7 kB 49%] [Connected to develo\u001b[0m\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\u001b[33m\r0% [Waiting for headers] [1 InRelease 43.1 kB/88.7 kB 49%] [Waiting for headers\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [1 InRelease 43.1 kB/88.7 k\u001b[0m\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [4 InRelease 14.2 kB/88.7 kB 16%] [1 InRelease 57\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [4 InRelease 15.6 kB/88.7 kB 18%] [Waiting for he\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rGet:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [5 InRelease 6,948 B/74.6 kB 9%] [Waiting for hea\u001b[0m\u001b[33m\r                                                                               \r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                         \rGet:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [6 InRelease 14.2 kB/15.9 k\u001b[0m\r                                                                               \rIgn:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","\u001b[33m\r                                                                               \r0% [2 InRelease gpgv 3,626 B] [6 InRelease 14.2 kB/15.9 kB 89%]\u001b[0m\r                                                               \rGet:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n","Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:13 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,937 kB]\n","Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,533 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,369 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,311 kB]\n","Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,093 kB]\n","Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [910 kB]\n","Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,073 kB]\n","Fetched 14.5 MB in 3s (4,185 kB/s)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","26 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","Note, selecting 'zlib1g-dev' instead of 'libz-dev'\n","cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n","xz-utils is already the newest version (5.2.2-1.3ubuntu0.1).\n","xz-utils set to manually installed.\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  zlib1g\n","The following NEW packages will be installed:\n","  zstd\n","The following packages will be upgraded:\n","  zlib1g zlib1g-dev\n","2 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n","Need to get 511 kB of archives.\n","After this operation, 1,141 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 zlib1g-dev amd64 1:1.2.11.dfsg-0ubuntu2.2 [176 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 zlib1g amd64 1:1.2.11.dfsg-0ubuntu2.2 [56.9 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 zstd amd64 1.3.3+dfsg-2ubuntu1.2 [278 kB]\n","Fetched 511 kB in 0s (4,112 kB/s)\n","(Reading database ... 155676 files and directories currently installed.)\n","Preparing to unpack .../zlib1g-dev_1%3a1.2.11.dfsg-0ubuntu2.2_amd64.deb ...\n","Unpacking zlib1g-dev:amd64 (1:1.2.11.dfsg-0ubuntu2.2) over (1:1.2.11.dfsg-0ubuntu2.1) ...\n","Preparing to unpack .../zlib1g_1%3a1.2.11.dfsg-0ubuntu2.2_amd64.deb ...\n","Unpacking zlib1g:amd64 (1:1.2.11.dfsg-0ubuntu2.2) over (1:1.2.11.dfsg-0ubuntu2.1) ...\n","Setting up zlib1g:amd64 (1:1.2.11.dfsg-0ubuntu2.2) ...\n","Selecting previously unselected package zstd.\n","(Reading database ... 155676 files and directories currently installed.)\n","Preparing to unpack .../zstd_1.3.3+dfsg-2ubuntu1.2_amd64.deb ...\n","Unpacking zstd (1.3.3+dfsg-2ubuntu1.2) ...\n","Setting up zstd (1.3.3+dfsg-2ubuntu1.2) ...\n","Setting up zlib1g-dev:amd64 (1:1.2.11.dfsg-0ubuntu2.2) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n"]}],"source":["!apt update\n","!DEBIAN_FRONTEND=noninteractive apt install -y cmake xz-utils zstd libz-dev"]},{"cell_type":"markdown","metadata":{"id":"003bJryBczW-"},"source":["#### Build library"]},{"cell_type":"markdown","metadata":{"id":"nPJbCJ-3czW_"},"source":["We downloaded the FasterTransformer code from Github to build this library manually and use additional scripts in it that allow us to convert pre-trained files of the GPT-J into FT binary format to be used at the inference time."]},{"cell_type":"markdown","metadata":{"id":"hhqS4MTpczW_"},"source":["Let's create a `build` directory that will be used as the main dir for the built library."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pm0oSP2QczW_","executionInfo":{"status":"ok","timestamp":1661323771879,"user_tz":-540,"elapsed":7,"user":{"displayName":"Solee Moon KR","userId":"15448835692925759007"}},"outputId":"5ecde6b3-5800-4f61-eecc-c6e94208abd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/FasterTransformer/build\n"]}],"source":["!mkdir -p build\n","%cd build "]},{"cell_type":"markdown","metadata":{"id":"lzHpTlsgczXA"},"source":["Let's build the library with the arg `-DBUILD_MULTI_GPU=ON ` that includes in the build pipeline multi-GPU cases like `GPT-J`."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCHn2SLKczXA","executionInfo":{"status":"ok","timestamp":1661325040855,"user_tz":-540,"elapsed":1268981,"user":{"displayName":"Solee Moon KR","userId":"15448835692925759007"}},"outputId":"6eaa4623-e16b-4278-b2d4-cbeb4efc331f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Submodule '3rdparty/Megatron-LM' (https://github.com/NVIDIA/Megatron-LM.git) registered for path '../3rdparty/Megatron-LM'\n","Submodule 'examples/tensorflow/bert/tensorflow_bert/bert' (https://github.com/google-research/bert.git) registered for path '../examples/tensorflow/bert/tensorflow_bert/bert'\n","Cloning into '/content/FasterTransformer/3rdparty/Megatron-LM'...\n","Cloning into '/content/FasterTransformer/examples/tensorflow/bert/tensorflow_bert/bert'...\n","Submodule path '../3rdparty/Megatron-LM': checked out '90e0a0dd08159e1c95f4f9d99bb8687f327d36c3'\n","Submodule path '../examples/tensorflow/bert/tensorflow_bert/bert': checked out 'eedf5716ce1268e56f0a50264a88cafad334ac61'\n","-- The CXX compiler identification is GNU 7.5.0\n","-- The CUDA compiler identification is NVIDIA 11.1.105\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/c++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Detecting CUDA compiler ABI info\n","-- Detecting CUDA compiler ABI info - done\n","-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n","-- Detecting CUDA compile features\n","-- Detecting CUDA compile features - done\n","-- Looking for C++ include pthread.h\n","-- Looking for C++ include pthread.h - found\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n","-- Looking for pthread_create in pthreads\n","-- Looking for pthread_create in pthreads - not found\n","-- Looking for pthread_create in pthread\n","-- Looking for pthread_create in pthread - found\n","-- Found Threads: TRUE  \n","-- Found CUDA: /usr/local/cuda (found suitable version \"11.1\", minimum required is \"10.2\") \n","\u001b[0mCUDA_VERSION 11 is greater or equal than 11, enable -DENABLE_BF16 flag\u001b[0m\n","-- Add DBUILD_MULTI_GPU, requires MPI and NCCL\n","-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version \"3.1\") \n","-- Found MPI: TRUE (found version \"3.1\")  \n","-- Found NCCL: /usr/include  \n","-- Determining NCCL version from /usr/include/nccl.h...\n","-- Looking for NCCL_VERSION_CODE\n","-- Looking for NCCL_VERSION_CODE - not found\n","-- Found NCCL (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libnccl.so)\n","\u001b[0m-- Assign GPU architecture (sm=70,75,80,86)\u001b[0m\n","\u001b[0m-- COMMON_HEADER_DIRS: /content/FasterTransformer;/usr/local/cuda/include\u001b[0m\n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/FasterTransformer/build\n","[  0%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/matrix_vector_multiplication.dir/matrix_vector_multiplication.cu.o\u001b[0m\n","[  0%] \u001b[32mBuilding CXX object src/fastertransformer/utils/CMakeFiles/cublasAlgoMap.dir/cublasAlgoMap.cc.o\u001b[0m\n","[  0%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/layernorm_kernels.dir/layernorm_kernels.cu.o\u001b[0m\n","[  1%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/logprob_kernels.dir/logprob_kernels.cu.o\u001b[0m\n","[  1%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/activation_kernels.dir/activation_kernels.cu.o\u001b[0m\n","[  0%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/add_residual_kernels.dir/add_residual_kernels.cu.o\u001b[0m\n","[  1%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/unfused_attention_kernels.dir/unfused_attention_kernels.cu.o\u001b[0m\n","[  1%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/decoder_masked_multihead_attention.dir/decoder_masked_multihead_attention.cu.o\u001b[0m\n","[  1%] \u001b[32mBuilding CUDA object src/fastertransformer/utils/CMakeFiles/memory_utils.dir/memory_utils.cu.o\u001b[0m\n","[  1%] \u001b[32mBuilding CXX object src/fastertransformer/utils/CMakeFiles/nccl_utils.dir/nccl_utils.cc.o\u001b[0m\n","[  1%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/ban_bad_words.dir/ban_bad_words.cu.o\u001b[0m\n","[  1%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/stop_criteria.dir/stop_criteria_kernels.cu.o\u001b[0m\n","[  2%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/custom_ar_kernels.dir/custom_ar_kernels.cu.o\u001b[0m\n","[  2%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/beam_search_penalty_kernels.dir/beam_search_penalty_kernels.cu.o\u001b[0m\n","[  3%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/calibrate_quantize_weight_kernels.dir/calibrate_quantize_weight_kernels.cu.o\u001b[0m\n","[  3%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/gpt_kernels.dir/gpt_kernels.cu.o\u001b[0m\n","[  3%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/beam_search_topk_kernels.dir/beam_search_topk_kernels.cu.o\u001b[0m\n","[  3%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/online_softmax_beamsearch_kernels.dir/online_softmax_beamsearch_kernels.cu.o\u001b[0m\n","[  3%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/decoding_kernels.dir/decoding_kernels.cu.o\u001b[0m\n","[  4%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/sampling_topp_kernels.dir/sampling_topp_kernels.cu.o\u001b[0m\n","[  4%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/sampling_topk_kernels.dir/sampling_topk_kernels.cu.o\u001b[0m\n","[  4%] \u001b[32mBuilding CXX object src/fastertransformer/utils/CMakeFiles/word_list.dir/word_list.cc.o\u001b[0m\n","[  4%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/transpose_int8_kernels.dir/transpose_int8_kernels.cu.o\u001b[0m\n","[  4%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/activation_int8_kernels.dir/activation_int8_kernels.cu.o\u001b[0m\n","[  4%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/bert_preprocess_kernels.dir/bert_preprocess_kernels.cu.o\u001b[0m\n","[  4%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/layernorm_int8_kernels.dir/layernorm_int8_kernels.cu.o\u001b[0m\n","[  4%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/sampling_penalty_kernels.dir/sampling_penalty_kernels.cu.o\u001b[0m\n","[  5%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/quantization_int8_kernels.dir/quantization_int8_kernels.cu.o\u001b[0m\n","[  6%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/layout_transformer_int8_kernels.dir/layout_transformer_int8_kernels.cu.o\u001b[0m\n","[  6%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/softmax_int8_kernels.dir/softmax_int8_kernels.cu.o\u001b[0m\n","[  6%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/matrix_transpose_kernels.dir/matrix_transpose_kernels.cu.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/cudaDriverWrapper.cpp.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_mha_with_relPosBias_fp16_128_32_kernel.sm75.cpp.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_mha_with_relPosBias_fp16_128_32_kernel.sm80.cpp.o\u001b[0m\n","[  7%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/nccl_utils.dir/cmake_device_link.o\u001b[0m\n","[  7%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/word_list.dir/cmake_device_link.o\u001b[0m\n","[  7%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libnccl_utils.a\u001b[0m\n","[  7%] Built target nccl_utils\n","[  7%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/longformer_kernels.dir/longformer_kernels.cu.o\u001b[0m\n","[  7%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libword_list.a\u001b[0m\n","[  7%] Built target word_list\n","[  8%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/add_bias_transpose_kernels.dir/add_bias_transpose_kernels.cu.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu(271): warning: invalid format string conversion\n","\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu(142): warning: variable \"input_val\" was declared but never referenced\n","          detected during instantiation of \"void fastertransformer::int8WeightPerChannelLdkMultiplicationLauncher(const int8_t *, const T *, const float *, T *, int, int, int, cudaStream_t) [with T=float]\" \n","(325): here\n","\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu(205): warning: variable \"input_val\" was declared but never referenced\n","          detected during instantiation of \"void fastertransformer::int8WeightPerChannelLdkMultiplicationLauncher(const int8_t *, const T *, const float *, T *, int, int, int, cudaStream_t) [with T=float]\" \n","(325): here\n","\n","/content/FasterTransformer/src/fastertransformer/kernels/decoding_kernels.cu(390): warning: variable \"selected_beam_index\" was set but never used\n","\n","[  8%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_mha_with_relPosBias_fp16_128_32_kernel.sm86.cpp.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[  8%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/cublasAlgoMap.dir/cmake_device_link.o\u001b[0m\n","[  8%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libcublasAlgoMap.a\u001b[0m\n","[  8%] Built target cublasAlgoMap\n","[  8%] \u001b[32mBuilding CXX object src/fastertransformer/utils/CMakeFiles/nvtx_utils.dir/nvtx_utils.cc.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_mha_with_relPosBias_fp16_256_32_kernel.sm75.cpp.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu(271): warning: invalid format string conversion\n","\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu(142): warning: variable \"input_val\" was declared but never referenced\n","          detected during instantiation of \"void fastertransformer::int8WeightPerChannelLdkMultiplicationLauncher(const int8_t *, const T *, const float *, T *, int, int, int, cudaStream_t) [with T=float]\" \n","(325): here\n","\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu(205): warning: variable \"input_val\" was declared but never referenced\n","          detected during instantiation of \"void fastertransformer::int8WeightPerChannelLdkMultiplicationLauncher(const int8_t *, const T *, const float *, T *, int, int, int, cudaStream_t) [with T=float]\" \n","(325): here\n","\n","[  8%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/nvtx_utils.dir/cmake_device_link.o\u001b[0m\n","[  9%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libnvtx_utils.a\u001b[0m\n","[  9%] Built target nvtx_utils\n","[  9%] \u001b[32mBuilding CXX object src/fastertransformer/utils/gemm_test/CMakeFiles/gemm_func.dir/gemm_func.cc.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_mha_with_relPosBias_fp16_256_32_kernel.sm80.cpp.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/kernels/decoding_kernels.cu(390): warning: variable \"selected_beam_index\" was set but never used\n","\n","/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu(532): warning: variable \"KEYS_PER_LDG\" was declared but never referenced\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu(271): warning: invalid format string conversion\n","\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu(142): warning: variable \"input_val\" was declared but never referenced\n","          detected during instantiation of \"void fastertransformer::int8WeightPerChannelLdkMultiplicationLauncher(const int8_t *, const T *, const float *, T *, int, int, int, cudaStream_t) [with T=float]\" \n","(325): here\n","\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu(205): warning: variable \"input_val\" was declared but never referenced\n","          detected during instantiation of \"void fastertransformer::int8WeightPerChannelLdkMultiplicationLauncher(const int8_t *, const T *, const float *, T *, int, int, int, cudaStream_t) [with T=float]\" \n","(325): here\n","\n","[ 10%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_mha_with_relPosBias_fp16_256_32_kernel.sm86.cpp.o\u001b[0m\n","[ 10%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/quantization_int8_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 11%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/matrix_transpose_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 11%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libquantization_int8_kernels.a\u001b[0m\n","[ 11%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libmatrix_transpose_kernels.a\u001b[0m\n","[ 11%] Built target quantization_int8_kernels\n","[ 11%] Built target matrix_transpose_kernels\n","[ 11%] \u001b[32mBuilding CXX object src/fastertransformer/utils/gemm_test/CMakeFiles/encoder_igemm_func.dir/encoder_igemm_func.cc.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/quantize_weight.dir/quantize_weight.cu.o\u001b[0m\n","[ 11%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/transpose_int8_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 11%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libtranspose_int8_kernels.a\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_mha_with_relPosBias_fp16_64_32_kernel.sm75.cpp.o\u001b[0m\n","[ 11%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/layout_transformer_int8_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 11%] Built target transpose_int8_kernels\n","[ 11%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/gen_relative_pos_bias.dir/gen_relative_pos_bias.cu.o\u001b[0m\n","[ 11%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/liblayout_transformer_int8_kernels.a\u001b[0m\n","[ 11%] Built target layout_transformer_int8_kernels\n","[ 11%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/transform_mask_kernels.dir/transform_mask_kernels.cu.o\u001b[0m\n","[ 11%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/gemm_func.dir/cmake_device_link.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_mha_with_relPosBias_fp16_64_32_kernel.sm80.cpp.o\u001b[0m\n","[ 12%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libgemm_func.a\u001b[0m\n","[ 12%] Built target gemm_func\n","/content/FasterTransformer/src/fastertransformer/kernels/decoding_kernels.cu(390): warning: variable \"selected_beam_index\" was set but never used\n","\n","[ 12%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/reverse_roll_kernels.dir/reverse_roll_kernels.cu.o\u001b[0m\n","[ 13%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/activation_int8_kernels.dir/cmake_device_link.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu(271): warning: invalid format string conversion\n","\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu(142): warning: variable \"input_val\" was declared but never referenced\n","          detected during instantiation of \"void fastertransformer::int8WeightPerChannelLdkMultiplicationLauncher(const int8_t *, const T *, const float *, T *, int, int, int, cudaStream_t) [with T=float]\" \n","(325): here\n","\n","[ 13%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libactivation_int8_kernels.a\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu(205): warning: variable \"input_val\" was declared but never referenced\n","          detected during instantiation of \"void fastertransformer::int8WeightPerChannelLdkMultiplicationLauncher(const int8_t *, const T *, const float *, T *, int, int, int, cudaStream_t) [with T=float]\" \n","(325): here\n","\n","[ 13%] Built target activation_int8_kernels\n","[ 13%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_mha_with_relPosBias_fp16_64_32_kernel.sm86.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/dequantize_kernels.dir/dequantize_kernels.cu.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 15%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_mha_with_relPosBias_int8_256_32_kernel.sm75.cpp.o\u001b[0m\n","[ 15%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/encoder_igemm_func.dir/cmake_device_link.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_mha_with_relPosBias_int8_256_32_kernel.sm80.cpp.o\u001b[0m\n","[ 15%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libencoder_igemm_func.a\u001b[0m\n","[ 15%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/add_bias_transpose_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 15%] Built target encoder_igemm_func\n","[ 15%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/unfused_attention_int8_kernels.dir/unfused_attention_int8_kernels.cu.o\u001b[0m\n","[ 15%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libadd_bias_transpose_kernels.a\u001b[0m\n","[ 15%] Built target add_bias_transpose_kernels\n","[ 15%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_mha_with_relPosBias_int8_64_32_kernel.sm75.cpp.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/kernels/decoding_kernels.cu(390): warning: variable \"selected_beam_index\" was set but never used\n","\n","[ 15%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/calibrate_quantize_weight_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_mha_with_relPosBias_int8_64_32_kernel.sm80.cpp.o\u001b[0m\n","[ 15%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libcalibrate_quantize_weight_kernels.a\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_fp16_128_64_kernel.sm75.cpp.o\u001b[0m\n","[ 16%] Built target calibrate_quantize_weight_kernels\n","[ 16%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/xlnet_preprocess_kernels.dir/xlnet_preprocess_kernels.cu.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_fp16_128_64_kernel.sm80.cpp.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 16%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_fp16_384_64_kernel.sm75.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_fp16_384_64_kernel.sm80.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_fp16_64_64_kernel.sm75.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_fp16_64_64_kernel.sm80.cpp.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu: In instantiation of ‘\u001b[01m\u001b[Kvoid fastertransformer::int8WeightPerChannelLdkMultiplicationLauncher(const int8_t*, const T*, const float*, T*, int, int, int, cudaStream_t) [with T = float; int8_t = signed char; cudaStream_t = CUstream_st*]\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu:325:205:\u001b[m\u001b[K   required from here\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu:269:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kconversion lacks type at end of format [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n"," \u001b[01;35m\u001b[K        printf(\"[ERROR][int8WeightPerChannelLdkMultiplicationLauncher] (%d % %d != 0) || (%d % 4 != 0).\\n\",\u001b[m\u001b[K\n","       \u001b[01;35m\u001b[K^\u001b[m\u001b[K \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K          \n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu:269:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunknown conversion type character ‘\u001b[01m\u001b[K \u001b[m\u001b[K’ in format [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu:288:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n","     whi\u001b[01;35m\u001b[Kle (block.x * 4 > k) \u001b[m\u001b[K{\n","        \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu: In instantiation of ‘\u001b[01m\u001b[Kvoid fastertransformer::int8WeightPerChannelLdkMultiplicationLauncher(const int8_t*, const T*, const float*, T*, int, int, int, cudaStream_t) [with T = __half; int8_t = signed char; cudaStream_t = CUstream_st*]\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu:334:203:\u001b[m\u001b[K   required from here\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu:269:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kconversion lacks type at end of format [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n"," \u001b[01;35m\u001b[K        printf(\"[ERROR][int8WeightPerChannelLdkMultiplicationLauncher] (%d % %d != 0) || (%d % 4 != 0).\\n\",\u001b[m\u001b[K\n","       \u001b[01;35m\u001b[K^\u001b[m\u001b[K \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K          \n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu:269:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunknown conversion type character ‘\u001b[01m\u001b[K \u001b[m\u001b[K’ in format [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu:288:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n","     whi\u001b[01;35m\u001b[Kle (block.x * 4 > k) \u001b[m\u001b[K{\n","        \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n","/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu: In instantiation of ‘\u001b[01m\u001b[Kvoid fastertransformer::int8WeightPerChannelLdkMultiplicationLauncher(const int8_t*, const T*, const float*, T*, int, int, int, cudaStream_t) [with T = __nv_bfloat16; int8_t = signed char; cudaStream_t = CUstream_st*]\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu:344:221:\u001b[m\u001b[K   required from here\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu:269:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kconversion lacks type at end of format [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n"," \u001b[01;35m\u001b[K        printf(\"[ERROR][int8WeightPerChannelLdkMultiplicationLauncher] (%d % %d != 0) || (%d % 4 != 0).\\n\",\u001b[m\u001b[K\n","       \u001b[01;35m\u001b[K^\u001b[m\u001b[K \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K          \n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu:269:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunknown conversion type character ‘\u001b[01m\u001b[K \u001b[m\u001b[K’ in format [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/matrix_vector_multiplication.cu:288:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n","     whi\u001b[01;35m\u001b[Kle (block.x * 4 > k) \u001b[m\u001b[K{\n","        \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n","[ 18%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/xlnet_attention_kernels.dir/xlnet_attention_kernels.cu.o\u001b[0m\n","[ 18%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/matrix_vector_multiplication.dir/cmake_device_link.o\u001b[0m\n","[ 19%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libmatrix_vector_multiplication.a\u001b[0m\n","[ 19%] Built target matrix_vector_multiplication\n","[ 19%] \u001b[32mBuilding CUDA object src/fastertransformer/kernels/CMakeFiles/vit_kernels.dir/vit_kernels.cu.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_fp16_96_64_kernel.sm75.cpp.o\u001b[0m\n","[ 19%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/quantize_weight.dir/cmake_device_link.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_fp16_96_64_kernel.sm80.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_int8_128_64_kernel.sm75.cpp.o\u001b[0m\n","[ 20%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libquantize_weight.a\u001b[0m\n","[ 20%] Built target quantize_weight\n","[ 20%] \u001b[32mBuilding CXX object examples/cpp/xlnet/CMakeFiles/cnpy.dir/cnpy.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_int8_128_64_kernel.sm80.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_int8_384_64_kernel.sm75.cpp.o\u001b[0m\n","[ 21%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/stop_criteria.dir/cmake_device_link.o\u001b[0m\n","[ 22%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/softmax_int8_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_int8_384_64_kernel.sm80.cpp.o\u001b[0m\n","[ 22%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/ban_bad_words.dir/cmake_device_link.o\u001b[0m\n","[ 22%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libstop_criteria.a\u001b[0m\n","[ 22%] Built target stop_criteria\n","[ 22%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libsoftmax_int8_kernels.a\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object examples/cpp/multi_gpu_gpt/CMakeFiles/gpt_example_utils.dir/gpt_example_utils.cc.o\u001b[0m\n","[ 22%] Built target softmax_int8_kernels\n","[ 22%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libban_bad_words.a\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object src/fastertransformer/utils/CMakeFiles/cublasMMWrapper.dir/cublasMMWrapper.cc.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_128_64_kernel.sm70.cpp.o\u001b[0m\n","[ 22%] Built target ban_bad_words\n","[ 22%] \u001b[32mBuilding CXX object src/fastertransformer/utils/gemm_test/CMakeFiles/encoder_gemm_func.dir/encoder_gemm_func.cc.o\u001b[0m\n","[ 23%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/sampling_penalty_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 23%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libsampling_penalty_kernels.a\u001b[0m\n","[ 23%] Built target sampling_penalty_kernels\n","[ 23%] \u001b[32mBuilding CXX object src/fastertransformer/utils/gemm_test/CMakeFiles/decoding_gemm_func.dir/decoding_gemm_func.cc.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 23%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_128_64_kernel.sm75.cpp.o\u001b[0m\n","[ 23%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/activation_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 23%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/add_residual_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 23%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/bert_preprocess_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 23%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libactivation_kernels.a\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_128_64_kernel.sm80.cpp.o\u001b[0m\n","[ 23%] Built target activation_kernels\n","[ 24%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libadd_residual_kernels.a\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object src/fastertransformer/utils/gemm_test/CMakeFiles/gpt_gemm_func.dir/gpt_gemm_func.cc.o\u001b[0m\n","[ 24%] Built target add_residual_kernels\n","[ 24%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libgpt_example_utils.a\u001b[0m\n","[ 24%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libbert_preprocess_kernels.a\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object src/fastertransformer/utils/gemm_test/CMakeFiles/xlnet_gemm_func.dir/xlnet_gemm_func.cc.o\u001b[0m\n","[ 24%] Built target gpt_example_utils\n","[ 24%] Built target bert_preprocess_kernels\n","[ 25%] \u001b[32mBuilding CXX object src/fastertransformer/utils/gemm_test/CMakeFiles/t5_gemm_func.dir/t5_gemm_func.cc.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object src/fastertransformer/utils/gemm_test/CMakeFiles/swin_igemm_func.dir/swin_igemm_func.cc.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_128_64_kernel.sm86.cpp.o\u001b[0m\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/custom_ar_kernels.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid fastertransformer::kernelLaunchConfig(int&, int&, size_t, int, size_t)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/custom_ar_kernels.cu:303:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n","     \u001b[01;35m\u001b[K        if (elts < (elts_per_th\u001b[m\u001b[Kread * DEFAULT_BLOCK_SIZE)) {  // local reduce\n","          \u001b[01;35m\u001b[K^\u001b[m\u001b[K  \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","[ 26%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_256_64_kernel.sm70.cpp.o\u001b[0m\n","[ 26%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/gen_relative_pos_bias.dir/cmake_device_link.o\u001b[0m\n","[ 26%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libgen_relative_pos_bias.a\u001b[0m\n","[ 27%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/transform_mask_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 27%] Built target gen_relative_pos_bias\n","[ 27%] \u001b[32mBuilding CXX object src/fastertransformer/utils/gemm_test/CMakeFiles/swin_gemm_func.dir/swin_gemm_func.cc.o\u001b[0m\n","[ 27%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/custom_ar_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 27%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libtransform_mask_kernels.a\u001b[0m\n","[ 27%] Built target transform_mask_kernels\n","[ 27%] \u001b[32mBuilding CXX object src/fastertransformer/layers/sampling_layers/CMakeFiles/BaseSamplingLayer.dir/BaseSamplingLayer.cc.o\u001b[0m\n","[ 28%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libcustom_ar_kernels.a\u001b[0m\n","[ 28%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/decoding_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_256_64_kernel.sm75.cpp.o\u001b[0m\n","[ 28%] Built target custom_ar_kernels\n","[ 28%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_256_64_kernel.sm80.cpp.o\u001b[0m\n","[ 29%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/encoder_gemm_func.dir/cmake_device_link.o\u001b[0m\n","[ 29%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libdecoding_kernels.a\u001b[0m\n","[ 29%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libencoder_gemm_func.a\u001b[0m\n","[ 29%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/cublasMMWrapper.dir/cmake_device_link.o\u001b[0m\n","[ 29%] Built target decoding_kernels\n","[ 29%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/reverse_roll_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_256_64_kernel.sm86.cpp.o\u001b[0m\n","[ 30%] Built target encoder_gemm_func\n","[ 30%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_384_64_kernel.sm70.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_384_64_kernel.sm75.cpp.o\u001b[0m\n","[ 31%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/decoding_gemm_func.dir/cmake_device_link.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_384_64_kernel.sm80.cpp.o\u001b[0m\n","[ 31%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libcublasMMWrapper.a\u001b[0m\n","[ 31%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libreverse_roll_kernels.a\u001b[0m\n","[ 31%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libdecoding_gemm_func.a\u001b[0m\n","[ 31%] Built target cublasMMWrapper\n","[ 31%] \u001b[32mBuilding CXX object src/fastertransformer/utils/CMakeFiles/cublasINT8MMWrapper.dir/cublasINT8MMWrapper.cc.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_384_64_kernel.sm86.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_64_64_kernel.sm70.cpp.o\u001b[0m\n","[ 32%] Built target reverse_roll_kernels\n","[ 32%] Built target decoding_gemm_func\n","[ 32%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_64_64_kernel.sm75.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_64_64_kernel.sm80.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_64_64_kernel.sm86.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_96_64_kernel.sm70.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_96_64_kernel.sm75.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_96_64_kernel.sm80.cpp.o\u001b[0m\n","[ 33%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/swin_igemm_func.dir/cmake_device_link.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_fp16_96_64_kernel.sm86.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_128_64_kernel.sm72.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_128_64_kernel.sm75.cpp.o\u001b[0m\n","[ 34%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libswin_igemm_func.a\u001b[0m\n","[ 34%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/beam_search_penalty_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 34%] Built target swin_igemm_func\n","[ 34%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_128_64_kernel.sm80.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_128_64_kernel.sm86.cpp.o\u001b[0m\n","[ 34%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libbeam_search_penalty_kernels.a\u001b[0m\n","[ 35%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/xlnet_gemm_func.dir/cmake_device_link.o\u001b[0m\n","[ 35%] Built target beam_search_penalty_kernels\n","[ 35%] \u001b[32mBuilding CUDA object src/fastertransformer/layers/beam_search_layers/CMakeFiles/BaseBeamSearchLayer.dir/BaseBeamSearchLayer.cu.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_192_64_kernel.sm72.cpp.o\u001b[0m\n","[ 35%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libxlnet_gemm_func.a\u001b[0m\n","[ 35%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/gpt_gemm_func.dir/cmake_device_link.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_192_64_kernel.sm75.cpp.o\u001b[0m\n","[ 35%] Built target xlnet_gemm_func\n","[ 36%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_192_64_kernel.sm80.cpp.o\u001b[0m\n","[ 36%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libgpt_gemm_func.a\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_192_64_kernel.sm86.cpp.o\u001b[0m\n","[ 36%] Built target gpt_gemm_func\n","[ 36%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_256_64_kernel.sm72.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_256_64_kernel.sm75.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_256_64_kernel.sm80.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_256_64_kernel.sm86.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_384_64_kernel.sm72.cpp.o\u001b[0m\n","[ 37%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/logprob_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 37%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/t5_gemm_func.dir/cmake_device_link.o\u001b[0m\n","[ 38%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/swin_gemm_func.dir/cmake_device_link.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_384_64_kernel.sm75.cpp.o\u001b[0m\n","[ 39%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/gpt_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_384_64_kernel.sm80.cpp.o\u001b[0m\n","[ 40%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/liblogprob_kernels.a\u001b[0m\n","[ 40%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libt5_gemm_func.a\u001b[0m\n","[ 40%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libswin_gemm_func.a\u001b[0m\n","[ 40%] Built target logprob_kernels\n","[ 40%] Built target t5_gemm_func\n","[ 40%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_384_64_kernel.sm86.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_64_64_kernel.sm75.cpp.o\u001b[0m\n","[ 41%] Built target swin_gemm_func\n","[ 41%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libgpt_kernels.a\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/fused_multihead_attention_v2_int8_64_64_kernel.sm80.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CUDA object 3rdparty/trt_fused_multihead_attention/CMakeFiles/trt_fused_multi_head_attention.dir/qkvToContext.cu.o\u001b[0m\n","[ 41%] Built target gpt_kernels\n","[ 42%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/cublasINT8MMWrapper.dir/cmake_device_link.o\u001b[0m\n","[ 42%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libcublasINT8MMWrapper.a\u001b[0m\n","[ 42%] Built target cublasINT8MMWrapper\n","[ 42%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/xlnet_attention_kernels.dir/cmake_device_link.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 42%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libxlnet_attention_kernels.a\u001b[0m\n","[ 42%] Built target xlnet_attention_kernels\n","[ 43%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/BaseSamplingLayer.dir/cmake_device_link.o\u001b[0m\n","[ 43%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libBaseSamplingLayer.a\u001b[0m\n","[ 43%] Built target BaseSamplingLayer\n","[ 44%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libcnpy.a\u001b[0m\n","[ 44%] Built target cnpy\n","[ 44%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/sampling_topk_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 44%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libsampling_topk_kernels.a\u001b[0m\n","[ 44%] Built target sampling_topk_kernels\n","[ 44%] \u001b[32mBuilding CUDA object src/fastertransformer/layers/sampling_layers/CMakeFiles/TopKSamplingLayer.dir/TopKSamplingLayer.cu.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 44%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/dequantize_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 44%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libdequantize_kernels.a\u001b[0m\n","[ 44%] Built target dequantize_kernels\n","[ 44%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/unfused_attention_int8_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 44%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/layernorm_int8_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 44%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libunfused_attention_int8_kernels.a\u001b[0m\n","[ 44%] Built target unfused_attention_int8_kernels\n","[ 44%] \u001b[32mBuilding CXX object src/fastertransformer/layers/attention_layers_int8/CMakeFiles/UnfusedAttentionLayerINT8.dir/UnfusedAttentionLayerINT8.cc.o\u001b[0m\n","[ 44%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/liblayernorm_int8_kernels.a\u001b[0m\n","[ 44%] Built target layernorm_int8_kernels\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 44%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/xlnet_preprocess_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 45%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libxlnet_preprocess_kernels.a\u001b[0m\n","[ 45%] Built target xlnet_preprocess_kernels\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 45%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/vit_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 46%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libvit_kernels.a\u001b[0m\n","[ 46%] Built target vit_kernels\n","[ 46%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/UnfusedAttentionLayerINT8.dir/cmake_device_link.o\u001b[0m\n","[ 46%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libUnfusedAttentionLayerINT8.a\u001b[0m\n","[ 46%] Built target UnfusedAttentionLayerINT8\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 46%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/longformer_kernels.dir/cmake_device_link.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 46%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/liblongformer_kernels.a\u001b[0m\n","[ 46%] Built target longformer_kernels\n","[ 47%] \u001b[32mBuilding CXX object src/fastertransformer/layers/attention_layers/CMakeFiles/LongformerAttentionLayer.dir/LongformerAttentionLayer.cc.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu(532): warning: variable \"KEYS_PER_LDG\" was declared but never referenced\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 47%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/LongformerAttentionLayer.dir/cmake_device_link.o\u001b[0m\n","[ 47%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libLongformerAttentionLayer.a\u001b[0m\n","[ 47%] Built target LongformerAttentionLayer\n","[ 47%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/BaseBeamSearchLayer.dir/cmake_device_link.o\u001b[0m\n","[ 47%] \u001b[32m\u001b[1mLinking CUDA static library ../../../../lib/libBaseBeamSearchLayer.a\u001b[0m\n","[ 47%] Built target BaseBeamSearchLayer\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid fastertransformer::set_alpha(uint32_t&, float, fastertransformer::Data_type)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:32:48:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kdereferencing type-punned pointer will break strict-aliasing rules [\u001b[01;35m\u001b[K-Wstrict-aliasing\u001b[m\u001b[K]\n","         alpha = reinterpret_cast<const uint32_t\u001b[01;35m\u001b[K&\u001b[m\u001b[K>(h2);\n","                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:36:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kdereferencing type-punned pointer will break strict-aliasing rules [\u001b[01;35m\u001b[K-Wstrict-aliasing\u001b[m\u001b[K]\n","         alpha = reinterpret_cast<const uint32_t&>\u001b[01;35m\u001b[K(\u001b[m\u001b[Knorm);\n","                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid fastertransformer::FusedMHARunnerInt8v2::mhaImpl::setup(int, int, int)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:416:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kdereferencing type-punned pointer will break strict-aliasing rules [\u001b[01;35m\u001b[K-Wstrict-aliasing\u001b[m\u001b[K]\n","         params.scale_bmm1 = reinterpret_cast<const uint32_t&>(scaleBmm\u001b[01;35m\u001b[K1\u001b[m\u001b[K);\n","                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:417:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kdereferencing type-punned pointer will break strict-aliasing rules [\u001b[01;35m\u001b[K-Wstrict-aliasing\u001b[m\u001b[K]\n","         params.scale_bmm2 = reinterpret_cast<const uint32_t&>(scaleBmm\u001b[01;35m\u001b[K2\u001b[m\u001b[K);\n","                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:418:77:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kdereferencing type-punned pointer will break strict-aliasing rules [\u001b[01;35m\u001b[K-Wstrict-aliasing\u001b[m\u001b[K]\n","         params.scale_softmax = reinterpret_cast<const uint32_t&>(scaleSoftma\u001b[01;35m\u001b[Kx\u001b[m\u001b[K);\n","                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid fastertransformer::FusedMHARunnerInt8v2::mhaImpl::setup(int, int)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:498:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kdereferencing type-punned pointer will break strict-aliasing rules [\u001b[01;35m\u001b[K-Wstrict-aliasing\u001b[m\u001b[K]\n","         params.scale_bmm1 = reinterpret_cast<const uint32_t&>(scaleBmm\u001b[01;35m\u001b[K1\u001b[m\u001b[K);\n","                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:499:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kdereferencing type-punned pointer will break strict-aliasing rules [\u001b[01;35m\u001b[K-Wstrict-aliasing\u001b[m\u001b[K]\n","         params.scale_bmm2 = reinterpret_cast<const uint32_t&>(scaleBmm\u001b[01;35m\u001b[K2\u001b[m\u001b[K);\n","                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:500:77:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kdereferencing type-punned pointer will break strict-aliasing rules [\u001b[01;35m\u001b[K-Wstrict-aliasing\u001b[m\u001b[K]\n","         params.scale_softmax = reinterpret_cast<const uint32_t&>(scaleSoftma\u001b[01;35m\u001b[Kx\u001b[m\u001b[K);\n","                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvirtual void fastertransformer::FusedMHARunnerInt8v2::setup(int, int, int)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:368:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kwarps_n\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n","         size_t w\u001b[01;35m\u001b[Karps_m,\u001b[m\u001b[K warps_n, warps_k = 1;\n","                 \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:368:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kwarps_m\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n","        \u001b[01;35m\u001b[K size_t\u001b[m\u001b[K warps_m, warps_n, warps_k = 1;\n","        \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvirtual void fastertransformer::FusedMHARunnerInt8v2::setup(int, int)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:449:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kwarps_n\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n","         size_t w\u001b[01;35m\u001b[Karps_m,\u001b[m\u001b[K warps_n, warps_k = 1;\n","                 \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[K/content/FasterTransformer/3rdparty/trt_fused_multihead_attention/qkvToContext.cu:449:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kwarps_m\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n","        \u001b[01;35m\u001b[K size_t\u001b[m\u001b[K warps_m, warps_n, warps_k = 1;\n","        \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n","/content/FasterTransformer/src/fastertransformer/kernels/unfused_attention_kernels.cu: In instantiation of ‘\u001b[01m\u001b[Kvoid fastertransformer::invokeTransposeQKV(T*, T*, int, int, int, int, cudaStream_t) [with T = __half; cudaStream_t = CUstream_st*]\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/unfused_attention_kernels.cu:765:163:\u001b[m\u001b[K   required from here\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/unfused_attention_kernels.cu:729:37:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n","         F\u001b[01;35m\u001b[KT_CHECK(grid.x * seq_per_block == batch_size * head_num * seq_len\u001b[m\u001b[K);\n","          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","/content/FasterTransformer/src/fastertransformer/kernels/unfused_attention_kernels.cu: In instantiation of ‘\u001b[01m\u001b[Kvoid fastertransformer::invokeTransposeQKV(T*, T*, int, int, int, int, cudaStream_t) [with T = __nv_bfloat16; cudaStream_t = CUstream_st*]\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/unfused_attention_kernels.cu:774:181:\u001b[m\u001b[K   required from here\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/unfused_attention_kernels.cu:729:37:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n","[ 47%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/trt_fused_multi_head_attention.dir/cmake_device_link.o\u001b[0m\n","[ 48%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libtrt_fused_multi_head_attention.a\u001b[0m\n","[ 48%] Built target trt_fused_multi_head_attention\n","[ 49%] \u001b[32mBuilding CUDA object src/fastertransformer/layers/attention_layers_int8/CMakeFiles/FusedAttentionLayerINT8.dir/FusedAttentionLayerINT8.cu.o\u001b[0m\n","[ 50%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/unfused_attention_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 50%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libunfused_attention_kernels.a\u001b[0m\n","[ 50%] Built target unfused_attention_kernels\n","[ 50%] \u001b[32mBuilding CXX object src/fastertransformer/layers/attention_layers/CMakeFiles/WindowAttention.dir/WindowAttention.cc.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 51%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/TopKSamplingLayer.dir/cmake_device_link.o\u001b[0m\n","[ 51%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libTopKSamplingLayer.a\u001b[0m\n","[ 51%] Built target TopKSamplingLayer\n","[ 51%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/WindowAttention.dir/cmake_device_link.o\u001b[0m\n","[ 51%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libWindowAttention.a\u001b[0m\n","[ 51%] Built target WindowAttention\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 52%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/layernorm_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 52%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/liblayernorm_kernels.a\u001b[0m\n","[ 52%] Built target layernorm_kernels\n","[ 52%] \u001b[32mBuilding CXX object src/fastertransformer/models/swin/CMakeFiles/SwinBlock.dir/SwinBlock.cc.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CUDA object src/fastertransformer/layers/attention_layers_int8/CMakeFiles/WindowAttentionINT8.dir/WindowAttentionINT8.cu.o\u001b[0m\n","[ 52%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/memory_utils.dir/cmake_device_link.o\u001b[0m\n","[ 52%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libmemory_utils.a\u001b[0m\n","[ 52%] Built target memory_utils\n","[ 53%] \u001b[32mBuilding CXX object src/fastertransformer/layers/CMakeFiles/FfnLayer.dir/FfnLayer.cc.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CUDA object src/fastertransformer/layers/attention_layers/CMakeFiles/FusedAttentionLayer.dir/FusedAttentionLayer.cu.o\u001b[0m\n","[ 54%] \u001b[32mBuilding CXX object src/fastertransformer/utils/CMakeFiles/custom_ar_comm.dir/custom_ar_comm.cc.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/fastertransformer/utils/CMakeFiles/gemm.dir/gemm.cc.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/fastertransformer/models/gptj/CMakeFiles/GptJDecoderLayerWeight.dir/GptJDecoderLayerWeight.cc.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/fastertransformer/models/multi_gpu_gpt/CMakeFiles/ParallelGptDecoderLayerWeight.dir/ParallelGptDecoderLayerWeight.cc.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/fastertransformer/layers/attention_layers/CMakeFiles/GptContextAttentionLayer.dir/GptContextAttentionLayer.cc.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/fastertransformer/models/xlnet/CMakeFiles/xlnet_gemm.dir/xlnet_gemm.cc.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/fastertransformer/models/bert/CMakeFiles/bert_gemm.dir/bert_gemm.cc.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/fastertransformer/layers/CMakeFiles/FfnLayerINT8.dir/FfnLayerINT8.cc.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object src/fastertransformer/models/swin/CMakeFiles/swin_gemm.dir/swin_gemm.cc.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object src/fastertransformer/models/multi_gpu_gpt/CMakeFiles/gpt_gemm.dir/gpt_gemm.cc.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object src/fastertransformer/layers/xlnet_attention_layers/CMakeFiles/XlnetAttentionLayer.dir/XlnetAttentionLayer.cc.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object src/fastertransformer/models/decoding/CMakeFiles/decoding_gemm.dir/decoding_gemm.cc.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object src/fastertransformer/layers/attention_layers/CMakeFiles/UnfusedAttentionLayer.dir/UnfusedAttentionLayer.cc.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object src/fastertransformer/models/t5/CMakeFiles/t5_gemm.dir/t5_gemm.cc.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CUDA object tests/unittests/CMakeFiles/test_logprob_kernels.dir/test_logprob_kernels.cu.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object examples/cpp/decoding/CMakeFiles/layernorm_test.dir/layernorm_test.cc.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object src/fastertransformer/models/vit/CMakeFiles/vit_gemm.dir/vit_gemm.cc.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/examples/cpp/decoding/layernorm_test.cc: In instantiation of ‘\u001b[01m\u001b[Kvoid add_bias_residual_layernorm_test(int, int) [with T = __half]\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/examples/cpp/decoding/layernorm_test.cc:100:60:\u001b[m\u001b[K   required from here\n","\u001b[01m\u001b[K/content/FasterTransformer/examples/cpp/decoding/layernorm_test.cc:191:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C++ forbids converting a string constant to ‘\u001b[01m\u001b[Kchar*\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wwrite-strings\u001b[m\u001b[K]\n","     \u001b[01;35m\u001b[KcheckMat(output_baseline, output_opt, m * n, \"output_baseline vs output_opt\")\u001b[m\u001b[K;\n","     \u001b[01;35m\u001b[K~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[K/content/FasterTransformer/examples/cpp/decoding/layernorm_test.cc:192:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C++ forbids converting a string constant to ‘\u001b[01m\u001b[Kchar*\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wwrite-strings\u001b[m\u001b[K]\n","     \u001b[01;35m\u001b[KcheckMat(normed_output_baseline, normed_output_opt, m * n, \"normed_output_baseline vs normed_output_opt\")\u001b[m\u001b[K;\n","     \u001b[01;35m\u001b[K~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","/content/FasterTransformer/examples/cpp/decoding/layernorm_test.cc: In instantiation of ‘\u001b[01m\u001b[Kvoid add_bias_residual_layernorm_test(int, int) [with T = float]\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/examples/cpp/decoding/layernorm_test.cc:103:61:\u001b[m\u001b[K   required from here\n","\u001b[01m\u001b[K/content/FasterTransformer/examples/cpp/decoding/layernorm_test.cc:191:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C++ forbids converting a string constant to ‘\u001b[01m\u001b[Kchar*\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wwrite-strings\u001b[m\u001b[K]\n","     \u001b[01;35m\u001b[KcheckMat(output_baseline, output_opt, m * n, \"output_baseline vs output_opt\")\u001b[m\u001b[K;\n","     \u001b[01;35m\u001b[K~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[K/content/FasterTransformer/examples/cpp/decoding/layernorm_test.cc:192:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C++ forbids converting a string constant to ‘\u001b[01m\u001b[Kchar*\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wwrite-strings\u001b[m\u001b[K]\n","     \u001b[01;35m\u001b[KcheckMat(normed_output_baseline, normed_output_opt, m * n, \"normed_output_baseline vs normed_output_opt\")\u001b[m\u001b[K;\n","     \u001b[01;35m\u001b[K~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../../../bin/swin_gemm\u001b[0m\n","[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../../../../bin/gpt_gemm\u001b[0m\n","[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../../../../bin/bert_gemm\u001b[0m\n","[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../../../../bin/xlnet_gemm\u001b[0m\n","[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../../../../bin/decoding_gemm\u001b[0m\n","[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../../../../bin/vit_gemm\u001b[0m\n","[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/layernorm_test\u001b[0m\n","[ 59%] Built target gpt_gemm\n","[ 59%] Built target swin_gemm\n","[ 59%] Built target bert_gemm\n","[ 59%] Built target decoding_gemm\n","[ 59%] Built target xlnet_gemm\n","[ 59%] Built target vit_gemm\n","[ 59%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/SwinBlock.dir/cmake_device_link.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 59%] Built target layernorm_test\n","[ 60%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libSwinBlock.a\u001b[0m\n","[ 60%] Built target SwinBlock\n","[ 60%] \u001b[32mBuilding CXX object src/fastertransformer/models/swin/CMakeFiles/SwinBasicLayer.dir/SwinBasicLayer.cc.o\u001b[0m\n","[ 61%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/FfnLayerINT8.dir/cmake_device_link.o\u001b[0m\n","[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../../../bin/t5_gemm\u001b[0m\n","[ 61%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libFfnLayerINT8.a\u001b[0m\n","[ 61%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/GptJDecoderLayerWeight.dir/cmake_device_link.o\u001b[0m\n","[ 61%] Built target FfnLayerINT8\n","[ 61%] Built target t5_gemm\n","[ 61%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libGptJDecoderLayerWeight.a\u001b[0m\n","[ 61%] Built target GptJDecoderLayerWeight\n","[ 61%] \u001b[32mBuilding CXX object src/fastertransformer/models/gptj/CMakeFiles/GptJWeight.dir/GptJWeight.cc.o\u001b[0m\n","[ 61%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/custom_ar_comm.dir/cmake_device_link.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 61%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/XlnetAttentionLayer.dir/cmake_device_link.o\u001b[0m\n","[ 61%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libcustom_ar_comm.a\u001b[0m\n","[ 61%] Built target custom_ar_comm\n","[ 61%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libXlnetAttentionLayer.a\u001b[0m\n","[ 61%] Built target XlnetAttentionLayer\n","[ 61%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/gemm.dir/cmake_device_link.o\u001b[0m\n","[ 61%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/GptContextAttentionLayer.dir/cmake_device_link.o\u001b[0m\n","[ 61%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libgemm.a\u001b[0m\n","[ 61%] Built target gemm\n","[ 61%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/UnfusedAttentionLayer.dir/cmake_device_link.o\u001b[0m\n","[ 61%] \u001b[32mBuilding CUDA object tests/unittests/CMakeFiles/test_gemm.dir/test_gemm.cu.o\u001b[0m\n","[ 62%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libGptContextAttentionLayer.a\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 62%] Built target GptContextAttentionLayer\n","[ 62%] \u001b[32mBuilding CXX object src/fastertransformer/layers/attention_layers/CMakeFiles/TensorParallelGptContextAttentionLayer.dir/TensorParallelGptContextAttentionLayer.cc.o\u001b[0m\n","[ 63%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libUnfusedAttentionLayer.a\u001b[0m\n","[ 63%] Built target UnfusedAttentionLayer\n","[ 63%] \u001b[32mBuilding CXX object src/fastertransformer/layers/attention_layers/CMakeFiles/TensorParallelUnfusedAttentionLayer.dir/TensorParallelUnfusedAttentionLayer.cc.o\u001b[0m\n","[ 64%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/ParallelGptDecoderLayerWeight.dir/cmake_device_link.o\u001b[0m\n","[ 64%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/FfnLayer.dir/cmake_device_link.o\u001b[0m\n","[ 64%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libParallelGptDecoderLayerWeight.a\u001b[0m\n","[ 64%] Built target ParallelGptDecoderLayerWeight\n","[ 64%] \u001b[32mBuilding CXX object src/fastertransformer/models/multi_gpu_gpt/CMakeFiles/ParallelGptWeight.dir/ParallelGptWeight.cc.o\u001b[0m\n","[ 64%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libFfnLayer.a\u001b[0m\n","[ 64%] Built target FfnLayer\n","[ 64%] \u001b[32mBuilding CXX object src/fastertransformer/layers/CMakeFiles/TensorParallelReluFfnLayer.dir/TensorParallelReluFfnLayer.cc.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object src/fastertransformer/models/longformer/CMakeFiles/LongformerEncoder.dir/LongformerEncoder.cc.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object src/fastertransformer/layers/CMakeFiles/TensorParallelGeluFfnLayer.dir/TensorParallelGeluFfnLayer.cc.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object src/fastertransformer/models/xlnet/CMakeFiles/Xlnet.dir/Xlnet.cc.o\u001b[0m\n","[ 64%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/GptJWeight.dir/cmake_device_link.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 64%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libGptJWeight.a\u001b[0m\n","[ 64%] Built target GptJWeight\n","[ 64%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/SwinBasicLayer.dir/cmake_device_link.o\u001b[0m\n","[ 64%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libSwinBasicLayer.a\u001b[0m\n","[ 64%] Built target SwinBasicLayer\n","[ 64%] \u001b[32mBuilding CXX object src/fastertransformer/models/swin/CMakeFiles/Swin.dir/Swin.cc.o\u001b[0m\n","[ 64%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/TensorParallelUnfusedAttentionLayer.dir/cmake_device_link.o\u001b[0m\n","[ 65%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/TensorParallelGptContextAttentionLayer.dir/cmake_device_link.o\u001b[0m\n","[ 65%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libTensorParallelUnfusedAttentionLayer.a\u001b[0m\n","[ 65%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libTensorParallelGptContextAttentionLayer.a\u001b[0m\n","[ 65%] Built target TensorParallelUnfusedAttentionLayer\n","[ 65%] Built target TensorParallelGptContextAttentionLayer\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 65%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/TensorParallelReluFfnLayer.dir/cmake_device_link.o\u001b[0m\n","[ 65%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/TensorParallelGeluFfnLayer.dir/cmake_device_link.o\u001b[0m\n","[ 66%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libTensorParallelReluFfnLayer.a\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 66%] Built target TensorParallelReluFfnLayer\n","[ 66%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libTensorParallelGeluFfnLayer.a\u001b[0m\n","[ 66%] Built target TensorParallelGeluFfnLayer\n","[ 66%] \u001b[32mBuilding CXX object src/fastertransformer/models/multi_gpu_gpt/CMakeFiles/ParallelGptContextDecoder.dir/ParallelGptContextDecoder.cc.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object src/fastertransformer/models/gptj/CMakeFiles/GptJContextDecoder.dir/GptJContextDecoder.cc.o\u001b[0m\n","[ 67%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/LongformerEncoder.dir/cmake_device_link.o\u001b[0m\n","[ 67%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libLongformerEncoder.a\u001b[0m\n","[ 67%] Built target LongformerEncoder\n","[ 67%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/Xlnet.dir/cmake_device_link.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 67%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libXlnet.a\u001b[0m\n","[ 67%] Built target Xlnet\n","[ 67%] \u001b[32mBuilding CXX object examples/cpp/xlnet/CMakeFiles/xlnet_correctness_example.dir/xlnet_correctness_example.cc.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object examples/cpp/xlnet/CMakeFiles/xlnet_example.dir/xlnet_example.cc.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 68%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/ParallelGptWeight.dir/cmake_device_link.o\u001b[0m\n","[ 69%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/Swin.dir/cmake_device_link.o\u001b[0m\n","[ 69%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libParallelGptWeight.a\u001b[0m\n","[ 69%] Built target ParallelGptWeight\n","[ 69%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libSwin.a\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 69%] Built target Swin\n","[ 70%] \u001b[32mBuilding CXX object examples/cpp/swin/CMakeFiles/swin_example.dir/swin_example.cc.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 71%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/GptJContextDecoder.dir/cmake_device_link.o\u001b[0m\n","[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libGptJContextDecoder.a\u001b[0m\n","[ 71%] Built target GptJContextDecoder\n","[ 71%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/ParallelGptContextDecoder.dir/cmake_device_link.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 72%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libParallelGptContextDecoder.a\u001b[0m\n","[ 72%] Built target ParallelGptContextDecoder\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 72%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/FusedAttentionLayerINT8.dir/cmake_device_link.o\u001b[0m\n","[ 72%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libFusedAttentionLayerINT8.a\u001b[0m\n","[ 72%] Built target FusedAttentionLayerINT8\n","[ 72%] \u001b[32mBuilding CXX object src/fastertransformer/models/vit_int8/CMakeFiles/ViTINT8.dir/ViTINT8.cc.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object src/fastertransformer/models/bert_int8/CMakeFiles/BertLayerINT8.dir/BertLayerINT8.cc.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/xlnet_example\u001b[0m\n","[ 72%] Built target xlnet_example\n","[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/xlnet_correctness_example\u001b[0m\n","[ 72%] Built target xlnet_correctness_example\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/swin_example\u001b[0m\n","[ 72%] Built target swin_example\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 72%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/BertLayerINT8.dir/cmake_device_link.o\u001b[0m\n","[ 72%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libBertLayerINT8.a\u001b[0m\n","[ 72%] Built target BertLayerINT8\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 72%] \u001b[32m\u001b[1mLinking CUDA executable ../../bin/test_logprob_kernels\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object src/fastertransformer/models/bert_int8/CMakeFiles/BertINT8.dir/BertINT8.cc.o\u001b[0m\n","[ 72%] Built target test_logprob_kernels\n","[ 72%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/ViTINT8.dir/cmake_device_link.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 73%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libViTINT8.a\u001b[0m\n","[ 73%] Built target ViTINT8\n","[ 73%] \u001b[32mBuilding CXX object examples/cpp/vit_int8/CMakeFiles/vit_int8_example.dir/vit_int8_example.cc.o\u001b[0m\n","[ 74%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/WindowAttentionINT8.dir/cmake_device_link.o\u001b[0m\n","[ 74%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libWindowAttentionINT8.a\u001b[0m\n","[ 74%] Built target WindowAttentionINT8\n","[ 74%] \u001b[32mBuilding CXX object src/fastertransformer/models/swin_int8/CMakeFiles/SwinBlockINT8.dir/SwinBlockINT8.cc.o\u001b[0m\n","[ 74%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/BertINT8.dir/cmake_device_link.o\u001b[0m\n","[ 75%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libBertINT8.a\u001b[0m\n","[ 75%] Built target BertINT8\n","[ 76%] \u001b[32mBuilding CXX object examples/cpp/bert_int8/CMakeFiles/bert_int8_example.dir/bert_int8_example.cc.o\u001b[0m\n","[ 76%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/FusedAttentionLayer.dir/cmake_device_link.o\u001b[0m\n","[ 76%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libFusedAttentionLayer.a\u001b[0m\n","[ 76%] Built target FusedAttentionLayer\n","[ 76%] \u001b[32mBuilding CXX object src/fastertransformer/models/t5/CMakeFiles/T5Encoder.dir/T5Encoder.cc.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object src/fastertransformer/models/t5/CMakeFiles/T5Encoder.dir/T5EncoderWeight.cc.o\u001b[0m\n","[ 77%] \u001b[32mBuilding CXX object src/fastertransformer/models/bert/CMakeFiles/Bert.dir/Bert.cc.o\u001b[0m\n","[ 78%] \u001b[32mBuilding CXX object src/fastertransformer/models/t5/CMakeFiles/T5Encoder.dir/T5EncoderLayerWeight.cc.o\u001b[0m\n","[ 78%] \u001b[32mBuilding CXX object src/fastertransformer/models/vit/CMakeFiles/ViT.dir/ViT.cc.o\u001b[0m\n","[ 78%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/SwinBlockINT8.dir/cmake_device_link.o\u001b[0m\n","[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/vit_int8_example\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu(532): warning: variable \"KEYS_PER_LDG\" was declared but never referenced\n","\n","[ 78%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libSwinBlockINT8.a\u001b[0m\n","[ 78%] Built target SwinBlockINT8\n","[ 79%] \u001b[32mBuilding CXX object src/fastertransformer/models/swin_int8/CMakeFiles/SwinBasicLayerINT8.dir/SwinBasicLayerINT8.cc.o\u001b[0m\n","[ 79%] Built target vit_int8_example\n","[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/test_gemm\u001b[0m\n","[ 79%] Built target test_gemm\n","[ 79%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/SwinBasicLayerINT8.dir/cmake_device_link.o\u001b[0m\n","[ 79%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libSwinBasicLayerINT8.a\u001b[0m\n","[ 79%] Built target SwinBasicLayerINT8\n","[ 80%] \u001b[32mBuilding CXX object src/fastertransformer/models/swin_int8/CMakeFiles/SwinINT8.dir/SwinINT8.cc.o\u001b[0m\n","[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/bert_int8_example\u001b[0m\n","[ 80%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/Bert.dir/cmake_device_link.o\u001b[0m\n","[ 81%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/T5Encoder.dir/cmake_device_link.o\u001b[0m\n","[ 81%] Built target bert_int8_example\n","[ 81%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/ViT.dir/cmake_device_link.o\u001b[0m\n","[ 81%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libBert.a\u001b[0m\n","[ 81%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libT5Encoder.a\u001b[0m\n","[ 81%] Built target Bert\n","[ 81%] Built target T5Encoder\n","[ 81%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libViT.a\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object examples/cpp/bert/CMakeFiles/bert_example.dir/bert_example.cc.o\u001b[0m\n","[ 81%] Built target ViT\n","[ 81%] \u001b[32mBuilding CXX object examples/cpp/vit/CMakeFiles/vit_example.dir/vit_example.cc.o\u001b[0m\n","[ 81%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/SwinINT8.dir/cmake_device_link.o\u001b[0m\n","[ 81%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libSwinINT8.a\u001b[0m\n","[ 81%] Built target SwinINT8\n","[ 82%] \u001b[32mBuilding CXX object examples/cpp/swin_int8/CMakeFiles/swin_int8_example.dir/swin_int8_example.cc.o\u001b[0m\n","[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/bert_example\u001b[0m\n","[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/vit_example\u001b[0m\n","[ 82%] Built target bert_example\n","[ 82%] Built target vit_example\n","[ 82%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/beam_search_topk_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 83%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libbeam_search_topk_kernels.a\u001b[0m\n","[ 83%] Built target beam_search_topk_kernels\n","[ 83%] \u001b[32mBuilding CUDA object src/fastertransformer/layers/beam_search_layers/CMakeFiles/BeamSearchLayer.dir/BeamSearchLayer.cu.o\u001b[0m\n","[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/swin_int8_example\u001b[0m\n","[ 83%] Built target swin_int8_example\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 83%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/BeamSearchLayer.dir/cmake_device_link.o\u001b[0m\n","[ 83%] \u001b[32m\u001b[1mLinking CUDA static library ../../../../lib/libBeamSearchLayer.a\u001b[0m\n","[ 83%] Built target BeamSearchLayer\n","/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu(532): warning: variable \"KEYS_PER_LDG\" was declared but never referenced\n","\n","/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu: In instantiation of ‘\u001b[01m\u001b[Kint fastertransformer::segmented_topp_impl::getSmemSizeAndCheck(const fastertransformer::segmented_topp_impl::TopKPerSegmentContext&, const fastertransformer::segmented_topp_impl::TopKPerSegmentParams&) [with Kernel_params = fastertransformer::segmented_topp_impl::Segmented_topk_kernel_params<float, int, 256, 2>]\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu:556:104:\u001b[m\u001b[K   required from here\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu:537:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n","     if (\u001b[01;35m\u001b[Ksmem_size + BLOCK_THREADS * sizeof(float) > context.sm_shared_size ||  // \u001b[m\u001b[Kdynamic + static memory\n","         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu: In instantiation of ‘\u001b[01m\u001b[Kint fastertransformer::segmented_topp_impl::getSmemSizeAndCheck(const fastertransformer::segmented_topp_impl::TopKPerSegmentContext&, const fastertransformer::segmented_topp_impl::TopKPerSegmentParams&) [with Kernel_params = fastertransformer::segmented_topp_impl::Segmented_topk_kernel_params<float, int, 256, 1>]\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu:559:104:\u001b[m\u001b[K   required from here\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu:537:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n","/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu: In instantiation of ‘\u001b[01m\u001b[Kint fastertransformer::segmented_topp_impl::getSmemSizeAndCheck(const fastertransformer::segmented_topp_impl::TopKPerSegmentContext&, const fastertransformer::segmented_topp_impl::TopKPerSegmentParams&) [with Kernel_params = fastertransformer::segmented_topp_impl::Segmented_topk_kernel_params<__half, int, 256, 4>]\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu:564:105:\u001b[m\u001b[K   required from here\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu:537:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n","/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu: In instantiation of ‘\u001b[01m\u001b[Kint fastertransformer::segmented_topp_impl::getSmemSizeAndCheck(const fastertransformer::segmented_topp_impl::TopKPerSegmentContext&, const fastertransformer::segmented_topp_impl::TopKPerSegmentParams&) [with Kernel_params = fastertransformer::segmented_topp_impl::Segmented_topk_kernel_params<__half, int, 256, 1>]\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu:567:105:\u001b[m\u001b[K   required from here\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/kernels/sampling_topp_kernels.cu:537:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n","[ 83%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/sampling_topp_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 83%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libsampling_topp_kernels.a\u001b[0m\n","[ 83%] Built target sampling_topp_kernels\n","[ 84%] \u001b[32mBuilding CUDA object src/fastertransformer/layers/sampling_layers/CMakeFiles/TopPSamplingLayer.dir/TopPSamplingLayer.cu.o\u001b[0m\n","[ 84%] \u001b[32mBuilding CUDA object src/fastertransformer/layers/sampling_layers/CMakeFiles/TopKTopPSamplingLayer.dir/TopKTopPSamplingLayer.cu.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 84%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/TopKTopPSamplingLayer.dir/cmake_device_link.o\u001b[0m\n","[ 84%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/TopPSamplingLayer.dir/cmake_device_link.o\u001b[0m\n","[ 84%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libTopKTopPSamplingLayer.a\u001b[0m\n","[ 84%] Built target TopKTopPSamplingLayer\n","[ 84%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libTopPSamplingLayer.a\u001b[0m\n","[ 84%] Built target TopPSamplingLayer\n","[ 84%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/online_softmax_beamsearch_kernels.dir/cmake_device_link.o\u001b[0m\n","[ 84%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libonline_softmax_beamsearch_kernels.a\u001b[0m\n","[ 84%] Built target online_softmax_beamsearch_kernels\n","[ 84%] \u001b[32mBuilding CUDA object src/fastertransformer/layers/beam_search_layers/CMakeFiles/OnlineBeamSearchLayer.dir/OnlineBeamSearchLayer.cu.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 84%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/OnlineBeamSearchLayer.dir/cmake_device_link.o\u001b[0m\n","[ 84%] \u001b[32m\u001b[1mLinking CUDA static library ../../../../lib/libOnlineBeamSearchLayer.a\u001b[0m\n","[ 84%] Built target OnlineBeamSearchLayer\n","[ 84%] \u001b[32mBuilding CXX object src/fastertransformer/layers/CMakeFiles/DynamicDecodeLayer.dir/DynamicDecodeLayer.cc.o\u001b[0m\n","[ 84%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/DynamicDecodeLayer.dir/cmake_device_link.o\u001b[0m\n","[ 84%] \u001b[32m\u001b[1mLinking CXX static library ../../../lib/libDynamicDecodeLayer.a\u001b[0m\n","[ 84%] Built target DynamicDecodeLayer\n","[ 84%] \u001b[32mBuilding CUDA object tests/unittests/CMakeFiles/test_sampling.dir/test_sampling.cu.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/utils/Tensor.h:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint fastertransformer::Tensor::getDataTypeByteNum(fastertransformer::DataType) const\u001b[m\u001b[K’:\n","\u001b[01m\u001b[K/content/FasterTransformer/src/fastertransformer/utils/Tensor.h:256:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcontrol reaches end of non-void function [\u001b[01;35m\u001b[K-Wreturn-type\u001b[m\u001b[K]\n"," \u001b[01;35m\u001b[K \u001b[m\u001b[K   }\n"," \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/test_sampling\u001b[0m\n","[ 84%] Built target test_sampling\n","[ 84%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/decoder_masked_multihead_attention.dir/cmake_device_link.o\u001b[0m\n","[ 84%] \u001b[32m\u001b[1mLinking CUDA static library ../../../lib/libdecoder_masked_multihead_attention.a\u001b[0m\n","[ 84%] Built target decoder_masked_multihead_attention\n","[ 84%] \u001b[32mBuilding CUDA object src/fastertransformer/layers/attention_layers/CMakeFiles/DecoderCrossAttentionLayer.dir/DecoderCrossAttentionLayer.cu.o\u001b[0m\n","[ 84%] \u001b[32mBuilding CXX object src/fastertransformer/layers/attention_layers/CMakeFiles/DecoderSelfAttentionLayer.dir/DecoderSelfAttentionLayer.cc.o\u001b[0m\n","[ 84%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/DecoderSelfAttentionLayer.dir/cmake_device_link.o\u001b[0m\n","[ 84%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libDecoderSelfAttentionLayer.a\u001b[0m\n","[ 84%] Built target DecoderSelfAttentionLayer\n","[ 84%] \u001b[32mBuilding CXX object src/fastertransformer/layers/attention_layers/CMakeFiles/TensorParallelDecoderSelfAttentionLayer.dir/TensorParallelDecoderSelfAttentionLayer.cc.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/layers/attention_layers/DecoderCrossAttentionLayer.cu(76): warning: specified alignment (2) is different from alignment (4) specified on a previous declaration\n","          detected during:\n","            instantiation of \"void fastertransformer::cross_attention_kernel(T *, const T *, T *, const T *, T *, const T *, const int *, T *, const __nv_bool *, int, int, int, int, int, T) [with T=half]\" \n","(443): here\n","            instantiation of \"void fastertransformer::cross_attention_dispatch(T *, const T *, T *, const T *, T *, const T *, const int *, T *, const __nv_bool *, int, int, int, int, int, int, __nv_bool, float, cudaStream_t) [with T=half]\" \n","(504): here\n","\n","[ 84%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/TensorParallelDecoderSelfAttentionLayer.dir/cmake_device_link.o\u001b[0m\n","[ 85%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libTensorParallelDecoderSelfAttentionLayer.a\u001b[0m\n","[ 85%] Built target TensorParallelDecoderSelfAttentionLayer\n","[ 85%] \u001b[32mBuilding CXX object src/fastertransformer/models/gptj/CMakeFiles/GptJDecoder.dir/GptJDecoder.cc.o\u001b[0m\n","[ 85%] \u001b[32mBuilding CXX object src/fastertransformer/models/multi_gpu_gpt/CMakeFiles/ParallelGptDecoder.dir/ParallelGptDecoder.cc.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/layers/attention_layers/DecoderCrossAttentionLayer.cu(76): warning: specified alignment (2) is different from alignment (4) specified on a previous declaration\n","          detected during:\n","            instantiation of \"void fastertransformer::cross_attention_kernel(T *, const T *, T *, const T *, T *, const T *, const int *, T *, const __nv_bool *, int, int, int, int, int, T) [with T=half]\" \n","(443): here\n","            instantiation of \"void fastertransformer::cross_attention_dispatch(T *, const T *, T *, const T *, T *, const T *, const int *, T *, const __nv_bool *, int, int, int, int, int, int, __nv_bool, float, cudaStream_t) [with T=half]\" \n","(504): here\n","\n","[ 85%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/GptJDecoder.dir/cmake_device_link.o\u001b[0m\n","[ 86%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libGptJDecoder.a\u001b[0m\n","[ 86%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/ParallelGptDecoder.dir/cmake_device_link.o\u001b[0m\n","[ 86%] Built target GptJDecoder\n","[ 86%] \u001b[32mBuilding CXX object src/fastertransformer/models/gptj/CMakeFiles/GptJ.dir/GptJ.cc.o\u001b[0m\n","[ 86%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libParallelGptDecoder.a\u001b[0m\n","[ 86%] Built target ParallelGptDecoder\n","[ 87%] \u001b[32mBuilding CXX object src/fastertransformer/models/multi_gpu_gpt/CMakeFiles/ParallelGpt.dir/ParallelGpt.cc.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/layers/attention_layers/DecoderCrossAttentionLayer.cu(76): warning: specified alignment (2) is different from alignment (4) specified on a previous declaration\n","          detected during:\n","            instantiation of \"void fastertransformer::cross_attention_kernel(T *, const T *, T *, const T *, T *, const T *, const int *, T *, const __nv_bool *, int, int, int, int, int, T) [with T=half]\" \n","(443): here\n","            instantiation of \"void fastertransformer::cross_attention_dispatch(T *, const T *, T *, const T *, T *, const T *, const int *, T *, const __nv_bool *, int, int, int, int, int, int, __nv_bool, float, cudaStream_t) [with T=half]\" \n","(504): here\n","\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","/content/FasterTransformer/src/fastertransformer/layers/attention_layers/DecoderCrossAttentionLayer.cu(76): warning: specified alignment (2) is different from alignment (4) specified on a previous declaration\n","          detected during:\n","            instantiation of \"void fastertransformer::cross_attention_kernel(T *, const T *, T *, const T *, T *, const T *, const int *, T *, const __nv_bool *, int, int, int, int, int, T) [with T=half]\" \n","(443): here\n","            instantiation of \"void fastertransformer::cross_attention_dispatch(T *, const T *, T *, const T *, T *, const T *, const int *, T *, const __nv_bool *, int, int, int, int, int, int, __nv_bool, float, cudaStream_t) [with T=half]\" \n","(504): here\n","\n","[ 87%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/GptJ.dir/cmake_device_link.o\u001b[0m\n","[ 87%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libGptJ.a\u001b[0m\n","[ 87%] Built target GptJ\n","[ 87%] \u001b[32mBuilding CXX object examples/cpp/gptj/CMakeFiles/gptj_example.dir/gptj_example.cc.o\u001b[0m\n","[ 87%] \u001b[32mBuilding CXX object src/fastertransformer/triton_backend/gptj/CMakeFiles/GptJTritonBackend.dir/GptJTritonModel.cc.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object src/fastertransformer/triton_backend/gptj/CMakeFiles/GptJTritonBackend.dir/GptJTritonModelInstance.cc.o\u001b[0m\n","/content/FasterTransformer/src/fastertransformer/utils/Tensor.h(256): warning: missing return statement at end of non-void function \"fastertransformer::Tensor::getDataTypeByteNum\"\n","\n","[ 88%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/ParallelGpt.dir/cmake_device_link.o\u001b[0m\n","[ 88%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libParallelGpt.a\u001b[0m\n","[ 88%] Built target ParallelGpt\n","[ 88%] \u001b[32mBuilding CXX object examples/cpp/multi_gpu_gpt/CMakeFiles/multi_gpu_gpt_async_example.dir/multi_gpu_gpt_async_example.cc.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object src/fastertransformer/triton_backend/multi_gpu_gpt/CMakeFiles/ParallelGptTritonBackend.dir/ParallelGptTritonModelInstance.cc.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object src/fastertransformer/triton_backend/multi_gpu_gpt/CMakeFiles/ParallelGptTritonBackend.dir/ParallelGptTritonModel.cc.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object examples/cpp/gpt/CMakeFiles/gpt_example.dir/gpt_example.cc.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object examples/cpp/multi_gpu_gpt/CMakeFiles/multi_gpu_gpt_example.dir/multi_gpu_gpt_example.cc.o\u001b[0m\n","[ 88%] \u001b[32m\u001b[1mLinking CXX shared library ../../../../lib/libGptJTritonBackend.so\u001b[0m\n","[ 88%] Built target GptJTritonBackend\n","[ 88%] \u001b[32mBuilding CXX object examples/cpp/gptj/CMakeFiles/gptj_triton_example.dir/gptj_triton_example.cc.o\u001b[0m\n","[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/gptj_example\u001b[0m\n","CMakeFiles/gptj_example.dir/gptj_example.cc.o: In function `MPI::Op::Init(void (*)(void const*, void*, int, MPI::Datatype const&), bool)':\n","gptj_example.cc:(.text._ZN3MPI2Op4InitEPFvPKvPviRKNS_8DatatypeEEb[_ZN3MPI2Op4InitEPFvPKvPviRKNS_8DatatypeEEb]+0x15): undefined reference to `ompi_mpi_cxx_op_intercept'\n","CMakeFiles/gptj_example.dir/gptj_example.cc.o: In function `MPI::Intracomm::Clone() const':\n","gptj_example.cc:(.text._ZNK3MPI9Intracomm5CloneEv[_ZNK3MPI9Intracomm5CloneEv]+0x3c): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/gptj_example.dir/gptj_example.cc.o: In function `MPI::Graphcomm::Clone() const':\n","gptj_example.cc:(.text._ZNK3MPI9Graphcomm5CloneEv[_ZNK3MPI9Graphcomm5CloneEv]+0x35): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/gptj_example.dir/gptj_example.cc.o: In function `MPI::Cartcomm::Sub(bool const*) const':\n","gptj_example.cc:(.text._ZNK3MPI8Cartcomm3SubEPKb[_ZNK3MPI8Cartcomm3SubEPKb]+0x9e): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/gptj_example.dir/gptj_example.cc.o: In function `MPI::Intracomm::Create_graph(int, int const*, int const*, bool) const':\n","gptj_example.cc:(.text._ZNK3MPI9Intracomm12Create_graphEiPKiS2_b[_ZNK3MPI9Intracomm12Create_graphEiPKiS2_b]+0x39): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/gptj_example.dir/gptj_example.cc.o: In function `MPI::Cartcomm::Clone() const':\n","gptj_example.cc:(.text._ZNK3MPI8Cartcomm5CloneEv[_ZNK3MPI8Cartcomm5CloneEv]+0x35): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/gptj_example.dir/gptj_example.cc.o:gptj_example.cc:(.text._ZNK3MPI9Intracomm11Create_cartEiPKiPKbb[_ZNK3MPI9Intracomm11Create_cartEiPKiPKbb]+0xa8): more undefined references to `MPI::Comm::Comm()' follow\n","CMakeFiles/gptj_example.dir/gptj_example.cc.o:(.data.rel.ro._ZTVN3MPI8DatatypeE[_ZTVN3MPI8DatatypeE]+0x78): undefined reference to `MPI::Datatype::Free()'\n","CMakeFiles/gptj_example.dir/gptj_example.cc.o:(.data.rel.ro._ZTVN3MPI3WinE[_ZTVN3MPI3WinE]+0x48): undefined reference to `MPI::Win::Free()'\n","collect2: error: ld returned 1 exit status\n","examples/cpp/gptj/CMakeFiles/gptj_example.dir/build.make:140: recipe for target 'bin/gptj_example' failed\n","make[2]: *** [bin/gptj_example] Error 1\n","CMakeFiles/Makefile2:5678: recipe for target 'examples/cpp/gptj/CMakeFiles/gptj_example.dir/all' failed\n","make[1]: *** [examples/cpp/gptj/CMakeFiles/gptj_example.dir/all] Error 2\n","make[1]: *** Waiting for unfinished jobs....\n","[ 88%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/DecoderCrossAttentionLayer.dir/cmake_device_link.o\u001b[0m\n","[ 89%] \u001b[32m\u001b[1mLinking CXX static library ../../../../lib/libDecoderCrossAttentionLayer.a\u001b[0m\n","[ 89%] Built target DecoderCrossAttentionLayer\n","[ 90%] \u001b[32m\u001b[1mLinking CXX shared library ../../../../lib/libParallelGptTritonBackend.so\u001b[0m\n","[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/gpt_example\u001b[0m\n","[ 91%] Built target ParallelGptTritonBackend\n","[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/multi_gpu_gpt_example\u001b[0m\n","[ 91%] Built target gpt_example\n","CMakeFiles/multi_gpu_gpt_example.dir/multi_gpu_gpt_example.cc.o: In function `MPI::Op::Init(void (*)(void const*, void*, int, MPI::Datatype const&), bool)':\n","multi_gpu_gpt_example.cc:(.text._ZN3MPI2Op4InitEPFvPKvPviRKNS_8DatatypeEEb[_ZN3MPI2Op4InitEPFvPKvPviRKNS_8DatatypeEEb]+0x15): undefined reference to `ompi_mpi_cxx_op_intercept'\n","CMakeFiles/multi_gpu_gpt_example.dir/multi_gpu_gpt_example.cc.o: In function `MPI::Intracomm::Clone() const':\n","multi_gpu_gpt_example.cc:(.text._ZNK3MPI9Intracomm5CloneEv[_ZNK3MPI9Intracomm5CloneEv]+0x3c): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/multi_gpu_gpt_example.dir/multi_gpu_gpt_example.cc.o: In function `MPI::Graphcomm::Clone() const':\n","multi_gpu_gpt_example.cc:(.text._ZNK3MPI9Graphcomm5CloneEv[_ZNK3MPI9Graphcomm5CloneEv]+0x35): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/multi_gpu_gpt_example.dir/multi_gpu_gpt_example.cc.o: In function `MPI::Cartcomm::Sub(bool const*) const':\n","multi_gpu_gpt_example.cc:(.text._ZNK3MPI8Cartcomm3SubEPKb[_ZNK3MPI8Cartcomm3SubEPKb]+0x9e): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/multi_gpu_gpt_example.dir/multi_gpu_gpt_example.cc.o: In function `MPI::Intracomm::Create_graph(int, int const*, int const*, bool) const':\n","multi_gpu_gpt_example.cc:(.text._ZNK3MPI9Intracomm12Create_graphEiPKiS2_b[_ZNK3MPI9Intracomm12Create_graphEiPKiS2_b]+0x39): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/multi_gpu_gpt_example.dir/multi_gpu_gpt_example.cc.o: In function `MPI::Cartcomm::Clone() const':\n","multi_gpu_gpt_example.cc:(.text._ZNK3MPI8Cartcomm5CloneEv[_ZNK3MPI8Cartcomm5CloneEv]+0x35): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/multi_gpu_gpt_example.dir/multi_gpu_gpt_example.cc.o:multi_gpu_gpt_example.cc:(.text._ZNK3MPI9Intracomm11Create_cartEiPKiPKbb[_ZNK3MPI9Intracomm11Create_cartEiPKiPKbb]+0xa8): more undefined references to `MPI::Comm::Comm()' follow\n","CMakeFiles/multi_gpu_gpt_example.dir/multi_gpu_gpt_example.cc.o:(.data.rel.ro._ZTVN3MPI8DatatypeE[_ZTVN3MPI8DatatypeE]+0x78): undefined reference to `MPI::Datatype::Free()'\n","CMakeFiles/multi_gpu_gpt_example.dir/multi_gpu_gpt_example.cc.o:(.data.rel.ro._ZTVN3MPI3WinE[_ZTVN3MPI3WinE]+0x48): undefined reference to `MPI::Win::Free()'\n","collect2: error: ld returned 1 exit status\n","examples/cpp/multi_gpu_gpt/CMakeFiles/multi_gpu_gpt_example.dir/build.make:142: recipe for target 'bin/multi_gpu_gpt_example' failed\n","make[2]: *** [bin/multi_gpu_gpt_example] Error 1\n","CMakeFiles/Makefile2:5806: recipe for target 'examples/cpp/multi_gpu_gpt/CMakeFiles/multi_gpu_gpt_example.dir/all' failed\n","make[1]: *** [examples/cpp/multi_gpu_gpt/CMakeFiles/multi_gpu_gpt_example.dir/all] Error 2\n","[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/gptj_triton_example\u001b[0m\n","CMakeFiles/gptj_triton_example.dir/gptj_triton_example.cc.o: In function `MPI::Op::Init(void (*)(void const*, void*, int, MPI::Datatype const&), bool)':\n","gptj_triton_example.cc:(.text._ZN3MPI2Op4InitEPFvPKvPviRKNS_8DatatypeEEb[_ZN3MPI2Op4InitEPFvPKvPviRKNS_8DatatypeEEb]+0x15): undefined reference to `ompi_mpi_cxx_op_intercept'\n","CMakeFiles/gptj_triton_example.dir/gptj_triton_example.cc.o: In function `MPI::Intracomm::Clone() const':\n","gptj_triton_example.cc:(.text._ZNK3MPI9Intracomm5CloneEv[_ZNK3MPI9Intracomm5CloneEv]+0x3c): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/gptj_triton_example.dir/gptj_triton_example.cc.o: In function `MPI::Graphcomm::Clone() const':\n","gptj_triton_example.cc:(.text._ZNK3MPI9Graphcomm5CloneEv[_ZNK3MPI9Graphcomm5CloneEv]+0x35): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/gptj_triton_example.dir/gptj_triton_example.cc.o: In function `MPI::Cartcomm::Sub(bool const*) const':\n","gptj_triton_example.cc:(.text._ZNK3MPI8Cartcomm3SubEPKb[_ZNK3MPI8Cartcomm3SubEPKb]+0x9e): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/gptj_triton_example.dir/gptj_triton_example.cc.o: In function `MPI::Intracomm::Create_graph(int, int const*, int const*, bool) const':\n","gptj_triton_example.cc:(.text._ZNK3MPI9Intracomm12Create_graphEiPKiS2_b[_ZNK3MPI9Intracomm12Create_graphEiPKiS2_b]+0x39): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/gptj_triton_example.dir/gptj_triton_example.cc.o: In function `MPI::Cartcomm::Clone() const':\n","gptj_triton_example.cc:(.text._ZNK3MPI8Cartcomm5CloneEv[_ZNK3MPI8Cartcomm5CloneEv]+0x35): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/gptj_triton_example.dir/gptj_triton_example.cc.o:gptj_triton_example.cc:(.text._ZNK3MPI9Intracomm11Create_cartEiPKiPKbb[_ZNK3MPI9Intracomm11Create_cartEiPKiPKbb]+0xa8): more undefined references to `MPI::Comm::Comm()' follow\n","CMakeFiles/gptj_triton_example.dir/gptj_triton_example.cc.o:(.data.rel.ro._ZTVN3MPI8DatatypeE[_ZTVN3MPI8DatatypeE]+0x78): undefined reference to `MPI::Datatype::Free()'\n","CMakeFiles/gptj_triton_example.dir/gptj_triton_example.cc.o:(.data.rel.ro._ZTVN3MPI3WinE[_ZTVN3MPI3WinE]+0x48): undefined reference to `MPI::Win::Free()'\n","collect2: error: ld returned 1 exit status\n","examples/cpp/gptj/CMakeFiles/gptj_triton_example.dir/build.make:102: recipe for target 'bin/gptj_triton_example' failed\n","make[2]: *** [bin/gptj_triton_example] Error 1\n","CMakeFiles/Makefile2:5709: recipe for target 'examples/cpp/gptj/CMakeFiles/gptj_triton_example.dir/all' failed\n","make[1]: *** [examples/cpp/gptj/CMakeFiles/gptj_triton_example.dir/all] Error 2\n","[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../../bin/multi_gpu_gpt_async_example\u001b[0m\n","CMakeFiles/multi_gpu_gpt_async_example.dir/multi_gpu_gpt_async_example.cc.o: In function `MPI::Op::Init(void (*)(void const*, void*, int, MPI::Datatype const&), bool)':\n","multi_gpu_gpt_async_example.cc:(.text._ZN3MPI2Op4InitEPFvPKvPviRKNS_8DatatypeEEb[_ZN3MPI2Op4InitEPFvPKvPviRKNS_8DatatypeEEb]+0x15): undefined reference to `ompi_mpi_cxx_op_intercept'\n","CMakeFiles/multi_gpu_gpt_async_example.dir/multi_gpu_gpt_async_example.cc.o: In function `MPI::Intracomm::Clone() const':\n","multi_gpu_gpt_async_example.cc:(.text._ZNK3MPI9Intracomm5CloneEv[_ZNK3MPI9Intracomm5CloneEv]+0x3c): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/multi_gpu_gpt_async_example.dir/multi_gpu_gpt_async_example.cc.o: In function `MPI::Graphcomm::Clone() const':\n","multi_gpu_gpt_async_example.cc:(.text._ZNK3MPI9Graphcomm5CloneEv[_ZNK3MPI9Graphcomm5CloneEv]+0x35): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/multi_gpu_gpt_async_example.dir/multi_gpu_gpt_async_example.cc.o: In function `MPI::Cartcomm::Sub(bool const*) const':\n","multi_gpu_gpt_async_example.cc:(.text._ZNK3MPI8Cartcomm3SubEPKb[_ZNK3MPI8Cartcomm3SubEPKb]+0x9e): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/multi_gpu_gpt_async_example.dir/multi_gpu_gpt_async_example.cc.o: In function `MPI::Intracomm::Create_graph(int, int const*, int const*, bool) const':\n","multi_gpu_gpt_async_example.cc:(.text._ZNK3MPI9Intracomm12Create_graphEiPKiS2_b[_ZNK3MPI9Intracomm12Create_graphEiPKiS2_b]+0x39): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/multi_gpu_gpt_async_example.dir/multi_gpu_gpt_async_example.cc.o: In function `MPI::Cartcomm::Clone() const':\n","multi_gpu_gpt_async_example.cc:(.text._ZNK3MPI8Cartcomm5CloneEv[_ZNK3MPI8Cartcomm5CloneEv]+0x35): undefined reference to `MPI::Comm::Comm()'\n","CMakeFiles/multi_gpu_gpt_async_example.dir/multi_gpu_gpt_async_example.cc.o:multi_gpu_gpt_async_example.cc:(.text._ZNK3MPI9Intracomm11Create_cartEiPKiPKbb[_ZNK3MPI9Intracomm11Create_cartEiPKiPKbb]+0xa8): more undefined references to `MPI::Comm::Comm()' follow\n","CMakeFiles/multi_gpu_gpt_async_example.dir/multi_gpu_gpt_async_example.cc.o:(.data.rel.ro._ZTVN3MPI8DatatypeE[_ZTVN3MPI8DatatypeE]+0x78): undefined reference to `MPI::Datatype::Free()'\n","CMakeFiles/multi_gpu_gpt_async_example.dir/multi_gpu_gpt_async_example.cc.o:(.data.rel.ro._ZTVN3MPI3WinE[_ZTVN3MPI3WinE]+0x48): undefined reference to `MPI::Win::Free()'\n","collect2: error: ld returned 1 exit status\n","examples/cpp/multi_gpu_gpt/CMakeFiles/multi_gpu_gpt_async_example.dir/build.make:142: recipe for target 'bin/multi_gpu_gpt_async_example' failed\n","make[2]: *** [bin/multi_gpu_gpt_async_example] Error 1\n","CMakeFiles/Makefile2:5877: recipe for target 'examples/cpp/multi_gpu_gpt/CMakeFiles/multi_gpu_gpt_async_example.dir/all' failed\n","make[1]: *** [examples/cpp/multi_gpu_gpt/CMakeFiles/multi_gpu_gpt_async_example.dir/all] Error 2\n","Makefile:135: recipe for target 'all' failed\n","make: *** [all] Error 2\n"]}],"source":["!git submodule init && git submodule update\n","!cmake -DSM=xx -DCMAKE_BUILD_TYPE=Release -DBUILD_MULTI_GPU=ON ..\n","!make -j32"]},{"cell_type":"markdown","metadata":{"id":"fbDgs-sAczXA"},"source":["Let's have a look at all pre-built binary files with examples and useful utilities."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UTL7OaP7czXB","executionInfo":{"status":"ok","timestamp":1661325041230,"user_tz":-540,"elapsed":380,"user":{"displayName":"Solee Moon KR","userId":"15448835692925759007"}},"outputId":"96e1fa42-1a8a-4c22-c10c-6bbb6b46ca0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["All built examples:\n","bert_example\t   swin_example\t\t vit_example\n","bert_gemm\t   swin_gemm\t\t vit_gemm\n","bert_int8_example  swin_int8_example\t vit_int8_example\n","decoding_gemm\t   t5_gemm\t\t xlnet_correctness_example\n","gpt_example\t   test_gemm\t\t xlnet_example\n","gpt_gemm\t   test_logprob_kernels  xlnet_gemm\n","layernorm_test\t   test_sampling\n"]}],"source":["print(\"All built examples:\")\n","!ls ./bin"]},{"cell_type":"markdown","metadata":{"id":"ftCNGx2kczXB"},"source":["Looks like the FasterTransformer library was successfully built and ready for the inference of our models."]},{"cell_type":"markdown","metadata":{"id":"UHrsOHgSczXB"},"source":["## 3.4 GPT-J 체크포인트 다운로드 및 구축하기"]},{"cell_type":"markdown","metadata":{"id":"6gDWucuMczXC"},"source":["C 프로그래밍 언어로 GPT를 실행하려면 사용자가 텐서플로우 또는 파이토치의 체크포인트를 바이너리 파일로 변환한 다음 FasterTransformer C API로 로드해야 합니다. 유감스럽게도 OpenAI에서 출시한 공개 대형 모델은 없습니다. 하지만 현재 FasterTransformer는 다른 회사의 여러 샘플을 제공하고 있습니다. 첫 번째는 Megatron-LM(PyTorch에 의해 훈련됨)의 체크포인트입니다. 다른 선택사항은 OpenAI GPT-2 모델(TensorFlow에 의해 훈련됨)의 체크포인트를 활용하는 것입니다. 그리고 이전 예에서 우리가 사용했던 것과 같은 Eleuther.AI 의 GPT-J 6b를 사용할 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"eoAwg8iBczXC"},"source":["가중치는 이미 다운로드해두었지만, 원하는 경우 언제든지 아래 줄에 대한 주석을 해제하여 다운로드할 수 있습니다."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u8EFnKyHczXC","executionInfo":{"status":"ok","timestamp":1661325041231,"user_tz":-540,"elapsed":11,"user":{"displayName":"Solee Moon KR","userId":"15448835692925759007"}},"outputId":"c0e22503-fbf9-4ced-9e68-4dd2a5ad2cfe"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/dli/task'\n","/content/FasterTransformer/build\n"]}],"source":["# Go to the main course dir\n","%cd /dli/task"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T02bENn1czXC","executionInfo":{"status":"ok","timestamp":1661325041231,"user_tz":-540,"elapsed":10,"user":{"displayName":"Solee Moon KR","userId":"15448835692925759007"}},"outputId":"5b5fa2b2-cbe6-4576-93df-b5604b4eacc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["tar: ./weights/gpt-j/ft/step_383500_slim.tar.zstd: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n"]}],"source":["# We already download weights for you to not to wait for them\n","# That's why these lines are commented\n","# !wget https://mystic.the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\n","!tar -axf ./weights/gpt-j/ft/step_383500_slim.tar.zstd -C ./model_repository/"]},{"cell_type":"markdown","metadata":{"id":"nFWJOZhsczXC"},"source":["다운로드된 가중치를 살펴보겠습니다."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dQh2Xn6XczXD","executionInfo":{"status":"ok","timestamp":1661325041232,"user_tz":-540,"elapsed":8,"user":{"displayName":"Solee Moon KR","userId":"15448835692925759007"}},"outputId":"929a26c1-cceb-490e-8135-95d370e1637b"},"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access './model_repository/step_383500/': No such file or directory\n"]}],"source":["!ls ./model_repository/step_383500/"]},{"cell_type":"markdown","metadata":{"id":"HlKUIE4lczXD"},"source":["가중치는 여러 샤드로 분할됩니다. 동일한 방법으로 Megatron  프레임워크로 사전 훈련을 받았습다. 샤드는 훈련 과정에서 다른 장치에 올려진 가중치의 일부입니다."]},{"cell_type":"markdown","metadata":{"id":"pbxMe-hBczXD"},"source":["이전 실습에서 사용했던 모델과 동일한 모델을 사용할 것이며, 토큰 처리 및 인코딩/디코딩을 위해 HuggingFace의 토크나이저를 사용할 것입니다. Huggingface에서 추론 단계에서 필요한 기본 Vocab 및 Merge 테이블 파일을 다운로드해야 합니다."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q72o6GLrczXD","executionInfo":{"status":"ok","timestamp":1661325041867,"user_tz":-540,"elapsed":640,"user":{"displayName":"Solee Moon KR","userId":"15448835692925759007"}},"outputId":"1d38bb87-257f-4e2a-e1ed-4d3281699dd5"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-08-24 07:10:41--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.37.30\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.37.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1042301 (1018K) [application/json]\n","Saving to: ‘models/gpt2-vocab.json’\n","\n","gpt2-vocab.json     100%[===================>]   1018K  --.-KB/s    in 0.03s   \n","\n","2022-08-24 07:10:41 (31.3 MB/s) - ‘models/gpt2-vocab.json’ saved [1042301/1042301]\n","\n","--2022-08-24 07:10:41--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.92.141\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.92.141|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 456318 (446K) [text/plain]\n","Saving to: ‘models/gpt2-merges.txt’\n","\n","gpt2-merges.txt     100%[===================>] 445.62K  --.-KB/s    in 0.02s   \n","\n","2022-08-24 07:10:41 (23.2 MB/s) - ‘models/gpt2-merges.txt’ saved [456318/456318]\n","\n"]}],"source":["!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json -P models\n","!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt -P models"]},{"cell_type":"markdown","metadata":{"id":"qASnezQyczXD"},"source":["## 3.5 추론을 위해 가중치를 FT 친화적인 포맷으로 변환"]},{"cell_type":"markdown","metadata":{"id":"Qwy6VZ7uczXD"},"source":["FasterTransformer의 C++ 백엔드에 대한 가중치는 이진 형식으로 변환되어야 합니다.</br>\n","FasterTransformer는 사전 훈련된 다양한 신경망을 위한 도구/스크립트를 제공합니. </br> \n","GPT-J 가중치의 경우 이 `FasterTransformer/examples/pytorch/gptj/utils/gptj_ckpt_convert.py` 스크립트를 사용하여 체크포인트를 변환할 수 있습니다.</br> \n","컨퍼터에는 다음 인자가 필요합니다. \n","\n","1. `--ckpt-dir`: megatron  모델의 경로\n","2.`--output-dir`: 변환된 모델의 출력 경로\n","3.`--n-inference-gpus`: 추론을 위한 텐서 병렬 크기"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0YaMwT6bczXE","executionInfo":{"status":"ok","timestamp":1661325041867,"user_tz":-540,"elapsed":8,"user":{"displayName":"Solee Moon KR","userId":"15448835692925759007"}},"outputId":"29b6cdaa-7447-49ce-ab2d-c08dbe559278"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'FasterTransformer/build'\n","/content/FasterTransformer/build\n"]}],"source":["# get to directoy with build FasterTransformer library \n","%cd FasterTransformer/build"]},{"cell_type":"code","execution_count":11,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"t9oSU4QwczXE","executionInfo":{"status":"ok","timestamp":1661325048639,"user_tz":-540,"elapsed":6777,"user":{"displayName":"Solee Moon KR","userId":"15448835692925759007"}},"outputId":"fec68eef-8517-4e80-eae8-fc7594167b6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["loading\n","loading shards for part 0\n","Traceback (most recent call last):\n","  File \"../examples/pytorch/gptj/utils/gptj_ckpt_convert.py\", line 276, in <module>\n","    checkpoint = main(in_path, num_layers)\n","  File \"../examples/pytorch/gptj/utils/gptj_ckpt_convert.py\", line 213, in main\n","    read_shard(f\"{ckpt_dir}shard_{i}/\", part) for i in range(total_shards)\n","  File \"../examples/pytorch/gptj/utils/gptj_ckpt_convert.py\", line 213, in <listcomp>\n","    read_shard(f\"{ckpt_dir}shard_{i}/\", part) for i in range(total_shards)\n","  File \"../examples/pytorch/gptj/utils/gptj_ckpt_convert.py\", line 73, in read_shard\n","    with open(file_path, \"rb\") as f:\n","FileNotFoundError: [Errno 2] No such file or directory: '../../model_repository/step_383500/shard_0/0.npz'\n"]}],"source":["# Converting weights for 1 GPU without parallelism\n","!python3 ../examples/pytorch/gptj/utils/gptj_ckpt_convert.py \\\n","                                --output-dir ../../models/j6b_ckpt \\\n","                                --ckpt-dir ../../model_repository/step_383500/ \\\n","                                --n-inference-gpus 1"]},{"cell_type":"markdown","metadata":{"id":"qj7p66xRczXE"},"source":["생성된 파일을 살펴보겠습니다. 우리는 조각난 가중치들을 추론에 사용하고자 하는 여러 GPU에 배치될 가중치 그룹으로 다시 결합했습니다. 지금은 GPU 1개를 사용하겠습니다."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b0bZkVeAczXE","executionInfo":{"status":"ok","timestamp":1661325048639,"user_tz":-540,"elapsed":22,"user":{"displayName":"Solee Moon KR","userId":"15448835692925759007"}},"outputId":"824a5114-f7f8-4835-9fad-5629163d69f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access '../../models/j6b_ckpt/': No such file or directory\n"]}],"source":["!ls ../../models/j6b_ckpt/"]},{"cell_type":"markdown","metadata":{"id":"Ow-Jvc0DczXE"},"source":["## 3.6 커널 자동 조정. 모델을 위한 가장 빠른 CUDA 커널 찾기## "]},{"cell_type":"markdown","metadata":{"id":"gC3XgVpyczXF"},"source":["이제 커널 오토 튜닝을 해야 합니다. </br>\n","행렬 곱은 트랜스포머 기반 신경망에서 가장 중요하고 무거운 작업입니다. FT는 CuBLAS 및 CuTLASS 라이브러리의 기능을 사용하여 이러한 유형의 작업을 실행합니다. MatMul 연산은 \"하드웨어\" 수준에서 서로 다른 로우레벨 알고리즘을 사용하여 수십 가지 다른 방식으로 실행될 수 있다는 것을 아는 것이 중요합니다. GemmBatchedEx 함수는 MatMul 연산을 구현하고 “cublasGemmAlgo_t” 를 입력 매개 변수로 가지고 있습니다. 이 매개 변수를 사용하여, 우리는 연산을 위해 다른 로우 레벨의 알고리즘을 선택할 수 있습니다. FasterTransformer 라이브러리는 이 매개 변수를 사용하여 모든 로우 레벨 알고리즘의 실시간 벤치마크를 수행하고 입력 데이터에 대한 모델의 매개 변수(어텐션 레이어 크기, 어텐션 헤드 수, 히든 레이어 크기 등)에 가장 적합한 매개 변수를 선택합니다. 또한 FT는__expf(), __shfl_xor_sync()</br></br>와 같은 일부 네트워크 부분에 하드웨어 가속 로우 레벨 함수를 사용합니다.\n","FasterTransformer 라이브러리를 구축하는 단계(2단계)에서 작성된 `./FasterTransformer/build/bin/gpt_gemm`바이너리 파일을 실행해 보겠습니다. 이 파일은 모델의 매개 변수와 다음과 같은 추가 매개 변수를 입력으로 사용합니다. \n","* `batch_size` \n","* `beam_width` \n","* `max_input_len` \n","* `head_number` \n","* `size_per_head`\n","* `inter_size`\n","* `vocab_size` \n","* `data_type` \n","* `tensor_para_size`\n","\n","이 모든 매개 변수는 신경망의 가장 무거운 부분을 시뮬레이션하고 가짜 가중치와 다중 로우 레벨 알고리즘으로 이러한 부분의 연산을 실행하는 데 필요하며, 실제 추론 과정에서 나중에 몇 가지 가장 빠른 알고리즘을 찾습니다. 이러한 매개 변수는 수동으로 설정해야 합니다. GPT-J 모델의 경우 FasterTransformer 팀은`../examples/cpp/gptj/gptj_config.ini` 구성 파일을 준비했습니다. 여기에는 GPT-J 모델에 대한 정보와 향후 추론에 대한 정보가 포함된 매개 변수가 포함되어 있습니다. </br>\n","<b>이러한 모든 테스트는 나중에 추론을 위해 사용될 GPU에서 실행되어야 합니다."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3LoqQdLCczXF","executionInfo":{"status":"ok","timestamp":1661325048984,"user_tz":-540,"elapsed":362,"user":{"displayName":"Solee Moon KR","userId":"15448835692925759007"}},"outputId":"7ef57a7c-d7f2-4b4f-d663-5134cbeb495e"},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] arguments: \n","  batch_size: 1 \n","  beam_width: 1 \n","  max_input_len: 128 \n","  head_num: 16 \n","  size_per_head: 256 \n","  inter_size: 16384 \n","  vocab_size: 50256 \n","  data_type: 1 \n","  tensor_para_size: 1 \n","\n","terminate called after throwing an instance of 'std::runtime_error'\n","  what():  [FT][ERROR] CUDA runtime error: no CUDA-capable device is detected /content/FasterTransformer/src/fastertransformer/models/multi_gpu_gpt/gpt_gemm.cc:69 \n","\n"]}],"source":["!CUDA_VISIBLE_DEVICES=1  ./bin/gpt_gemm 1 1 128 16 256 16384 50256 1 1"]},{"cell_type":"markdown","metadata":{"id":"mzf7rBjZczXF"},"source":["그 결과, 시도한 알고리즘과 각 알고리즘에 대한 실행 시간에 대한 정보가 들어 있는  `gemm_config.in` 파일을 디렉터리에 받았습니다.  이 구성 파일은 나중에 바인딩 또는 C++ 코드를 통한 추론 예제에서 FT 라이브러리에서 사용됩니다."]},{"cell_type":"markdown","metadata":{"id":"Y5pOtiLjczXF"},"source":["# 3.6 C++ 바인딩을 사용한 GPT-J 추론"]},{"cell_type":"markdown","metadata":{"id":"PgoEHMqBczXF"},"source":["## 3.6.1 1개의 GPU에서 추론하기 "]},{"cell_type":"markdown","metadata":{"id":"R2lUuclsczXF"},"source":["시작하기 전에 이전 과정에서 활용된`./FasterTransformer/examples/cpp/gptj/gptj_config.ini` 구성 파일을 한 번 더 살펴봐야 합니다.  여기에는 GPT 디코더 레이어 수, 헤드 수, MLP 레이어의 히든 뉴런 수 등과 같은 GPT-J 모델에 대한 주요 정보가 포함되어 있습니다. 동시에 배치 크기, 온도, top_k, top_p, beam_search 매개 변수 및 정밀도 모드(FP16 또는 FP32)와 같은 C++에서의 추론을 위한 하이퍼 매개 변수를 포함합니다."]},{"cell_type":"markdown","metadata":{"id":"65uv1_K3czXF"},"source":["`./FasterTransformer/examples/cpp/gptj/gptj_config.ini`를 구성하여 GPT-J C++ 추론 사례를 실행합니다.</br>\n","우리는 이 파일을 JupyterLab에서 열고 HuggingFace에서 했던 추론 파이프라인과 유사한 추론 사례를 만들기 위해 이 라인들을 업데이트해야 합니다. </br>\n","<b>`./FasterTransformer/examples/cpp/gptj/gptj_config.ini`를 주피터 노트북에서 엽니다. 다음 행을 변경합니다. </b>\n","* max_batch_size=<b>1</b> # HF 파이프라인과 유사하게 설정\n","* top_k=<b>50</b> # HF 파이프라인과 유사하게 설정\n","* top_p=<b>1</b> # HF 파이프라인과 유사하게 설정\n","* is_half=<b>1</b> #FP16 모드에서 모델 실행을 위해 1 로 설정(반정밀도), 아니면, GPU 가용 메모리가 충분치 않아 OOM 에러 발생\n","* tensor_para_size=<b>1</b> #텐서 병렬화 모드 없이 1개 GPU에서 구동\n","* model_dir=<b>../../models/j6b_ckpt/</b> # pre-baked 가중치 경로\n","* request_batch_size=<b>1</b>\n","* request_output_len=<b>120</b> # 출력길이는 HF와 유사하게 설정\n","\n","다른 모든 매개 변수는 파일에 표시된 것과 같아야 합니다."]},{"cell_type":"markdown","metadata":{"id":"TaFr4KUDczXG"},"source":["사전 빌드된 스크립트 `./FasterTransformer/build/bin/gptj_example`을 실행해 보겠습니다. `./FasterTransformer/examples/cpp/gptj/gptj_example.cpp`C++ 파일에서 구축했습니다. </br>\n","이 파일은 `./FasterTransformer/examples/cpp/gptj/gptj_config.ini`로부터 우리가 준비한 가중치와 구성을 적용합니다. 이 바이너리 파일은 `./FasterTransformer/examples/cpp/gptj/start_ids.csv` 로부터 입력 토큰을 프롬프트로 가져와 GPT-J의 작업 과정에서 생성된 토큰을 포함하는 `out`  텍스트 파일을 생성합니다."]},{"cell_type":"markdown","metadata":{"id":"RvyxjVPuczXG"},"source":["`CUDA_VISIBLE_DEVICES` 환경 변수를 사용하면 현재 태스크/프로세스/스크립트에 사용할 GPU를 선택할 수 있습니다. 시스템에 4개의 GPU가 있으며(0-3 숫자로 구성, 0부터 시작) 이 변수를 사용하여 GPU 1 (2번째 GPU)에서 현재 예제를 실행할 것입니다. 이전 작업(2번째 노트북)이 GPU 0에서 시작되었고 Jupyter 커널을 중지하지 않았다면 모델이 이미 존재하기 때문에 GPU 1을 사용할 것입니다. 그래서 우리는 OOM 오류를 피할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1i_cmhoczXG"},"outputs":[],"source":["!CUDA_VISIBLE_DEVICES=1 ./bin/gptj_example"]},{"cell_type":"markdown","metadata":{"id":"x8CpgyfCczXG"},"source":["우리는 길이가 같은 문장의 추론 시간이 <b>1.8 초</b>, 즉 기본 PyTorch + HuggingFace 파이프라인보다<b>x3.5</b> 배 더 빠름을 알 수 있습니다.</br>\n","`out`파일을 디코드하여 신경망에서 무엇이 생성되었는지 알아보겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dsFVuxmczXG"},"outputs":[],"source":["!python3 ../examples/pytorch/gpt/utils/gpt_token_converter.py \\\n","                       --vocab_file=../../models/gpt2-vocab.json  \\\n","                       --bpe_file=../../models/gpt2-merges.txt"]},{"cell_type":"markdown","metadata":{"id":"shBt5a1mczXG"},"source":["사용자들은`examples/cpp/multi_gpu_gpt/gpt_config.ini`에서 구성 인자들의 세부 정보를 볼 수 있습니다. 모델 경로, 모델 크기, 텐서 병렬 크기 및 일부 하이퍼 파라미터를 제어합니다."]},{"cell_type":"markdown","metadata":{"id":"KR7Q_B8DczXG"},"source":["## 3.6.2 2개의 GPU에서 추론하기"]},{"cell_type":"markdown","metadata":{"id":"RzHCWzXVczXG"},"source":["FasterTransformer에 통합된 텐서-병렬화 기술을 사용하여 동일한 추론 단계를 실행하지만 거의 두 배 더 빠르게 실행할 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"_FLgaHRYczXG"},"source":["텐서-병렬화 기법에 대해 좀 더 자세히 알아보겠습니다.\n","아래 그림에서 텐서-병렬화 기법의 기본 예를 볼 수 있습니다. 많은 신경망의 기본 연산인 단순 행렬 곱(MatMul) 연산은 여러 가지 방법으로 계산할 수 있습니다. 일부 방법에서는 행렬을 여러 조각으로 분할하고 여러 장치(GPU)에서 MatMul 연산의 일부를 병렬로 계산한 다음 결과를 집계할 수 있습니다. 즉, 2개의 GPU를 사용할 수 있고 그 사이에 빠르게 상호 연결할 수 있는 경우(NVLink/NVSwitch), 최대 2배 낮은 대기 시간으로 동일한 MatMul 연산을 병렬로 계산할 수 있습니다.\n","\n","<img src=\"./images/parallelism-tp-parallel_gemm.png\" style=\"width: 800px;\">"]},{"cell_type":"markdown","metadata":{"id":"8OqeH5OFczXH"},"source":["마지막 사진은 Hugging Face 사이트에서 찍은 사진입니다. https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-tp-parallel_gemm.png"]},{"cell_type":"markdown","metadata":{"id":"kGlF3cLEczXH"},"source":["동일한 기술을 사용하여 현재 Transformer 모델의 주요 블록인 Attention과 MLP 블록을 병렬화할 수 있습니다:\n","<img src=\"./images/parallelism-tp-parallel_self_attention.png\" style=\"width: 600px;\"></br>\n","<img src=\"./images/parallelism-tp-parallel_shard_processing.png\" style=\"width: 600px;\">"]},{"cell_type":"markdown","metadata":{"id":"DvXurRxyczXH"},"source":["텐서-병렬화=2 모드에서 GPT-J의 추론을 실행해 보겠습니다. 즉, 가중치를 두 부분으로 나누어 NVLink와 연결된 여러 GPU에 배치해야 합니다. FasterTransformer에는 스크립트에 특별한 인자들을 추가하였으며 우리는 아무것도 구현할 필요가 없습니다."]},{"cell_type":"markdown","metadata":{"id":"IhgN09o5czXH"},"source":["가중치 준비에 대해서는 동일한 스크립트를 사용하되 TP=2 모드에 대한 가중치를 준비하기 위해`--n-inference-gpus`를 `2` 로 설정합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"KI_2SHKrczXH"},"outputs":[],"source":["# Converting weights for 2 GPU without parallelism\n","!python3 ../examples/pytorch/gptj/utils/gptj_ckpt_convert.py \\\n","                                --output-dir ../../models/j6b_ckpt \\\n","                                --ckpt-dir ../../model_repository/step_383500/ \\\n","                                --n-inference-gpus 2"]},{"cell_type":"markdown","metadata":{"id":"PBLFdSDAczXH"},"source":["생성된 파일을 살펴보겠습니다. 우리는 조각난 가중치를 추론에 사용하고자 하는 다수의 GPU에 배치될 가중치 그룹으로 다시 결합했습니다. 일단 GPU 2개를 사용하겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IxVlC09_czXH"},"outputs":[],"source":["!ls ../../models/j6b_ckpt/"]},{"cell_type":"markdown","metadata":{"id":"RHgXzcowczXI"},"source":["이제 커널 오토 튜닝을 다시 실행하여 2xGPU 모드에서 가장 빠른 커널을 찾아야 합니다. 먼저 이전 단계에서 이미 1xGPU 모드에 대해 찾은 커널로 추론을 실행해 보겠습니다. 우리는 이 비최적화된 추론을 나중에 커널오토튜닝과 비교하여 이 작업이 제공하는 가속도를 보기 위해 수행합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JIZf0d9IczXL"},"outputs":[],"source":["!CUDA_VISIBLE_DEVICES=1,2 mpirun -n 2 --allow-run-as-root ./bin/gptj_example"]},{"cell_type":"markdown","metadata":{"id":"PTjfxIKsczXL"},"source":["최적 커널이 아닌 경우의 추론은 <b>~1.4초<b>입니다. 우리는 1xGPU 모드와 비교하여 가속을 달성했지만, 더 나은 결과를 낼 수 있습니다. 이제 다른 알고리즘이 더 잘 작동할 가능성이 있기 때문에 Tp 2 체제에 대한 커널 오토 튜닝을 수행해야 합니다. 신경망에 대해 동일한 매개 변수를 사용하지만, 이때`tensor_para_size = 2`로 설정해야 합니다."]},{"cell_type":"markdown","metadata":{"id":"iqDIUCHZczXM"},"source":["이전 작업(노트북 02)이 GPU 0에서 시작되었고 Jupyter 커널을 중지하지 않았다면 모델이 이미 있기 때문에 GPU 1을 사용할 것입니다. (OOM 오류를 피하기 위함입니다.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YVuLkpe8czXM"},"outputs":[],"source":["!CUDA_VISIBLE_DEVICES=1  ./bin/gpt_gemm 1 1 128 16 256 16384 50256 1 2"]},{"cell_type":"markdown","metadata":{"id":"5atXT-OnczXM"},"source":["첫 번째 랩에서 서버에서 4개의 GPU를 사용할 수 있다는 것을 기억합니다. 그 중 2개를 사용하여 TP=2 체제에서 추론을 시작하겠습니다."]},{"cell_type":"markdown","metadata":{"id":"kBtRCLTZczXM"},"source":["GPT-J C++ 추론 사례를 실행하기 위해 `./FasterTransformer/examples/cpp/gptj/gptj_config.ini`를 구성합니다.</br>\n","우리는 이 파일을 JupyterLab에서 열고 이 라인들을 업데이트하여 이전에 GPU가 2개인 추론 파이프라인과 유사한 추론 사례를 만들어야 합니다. </br>\n","* tensor_para_size=<b>2</b> #텐서 병렬화 모드로 2xGPUs에서 실행\n","\n","다른 모든 파라미터는 FT를 사용한 이전 실행과 동일해야 합니다.\n"," \n","이전 작업(노트북 02)이 GPU 0에서 시작되었고 Jupyter 커널을 중지하지 않았다면 모델이 이미 있기 때문에 GPU 2와 3(번호 1과 2)을 사용할 것입니다. OOM 오류를 피하기 위해서이며, 이제 최적화된 커널로 추론을 실행해 보겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIGQGthVczXM"},"outputs":[],"source":["!CUDA_VISIBLE_DEVICES=1,2 mpirun -n 2 --allow-run-as-root ./bin/gptj_example"]},{"cell_type":"markdown","metadata":{"id":"r9cq585MczXM"},"source":["동일한 길이의 문장의 추론 시간이 기본 PyTorch + HuggingFace 파이프라인보다 <b>x5</b>배 빠른 <b>1.25초 </b>임을 알 수 있습니다! 물론 여기서 2개의 GPU를 사용했지만, 이는 추론에 GPU를 더 추가하여 대기 시간을 단축할 수 있는 방법을 보여줍니다.</br>\n","`out` 파일을 디코딩하여 신경망에서 무엇이 생성되었는지 알아보겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsE0aHU7czXN"},"outputs":[],"source":["!python3 ../examples/pytorch/gpt/utils/gpt_token_converter.py \\\n","                               --vocab_file=../../models/gpt2-vocab.json  \\\n","                               --bpe_file=../../models/gpt2-merges.txt"]},{"cell_type":"markdown","metadata":{"id":"E4bdbnoEczXN"},"source":["## 결과"]},{"cell_type":"markdown","metadata":{"id":"ZITLdWw0czXN"},"source":["결과들을 비교해 보겠습니다! 우리가 볼 수 있듯이, 우리는 HuggingFace 와 텐서-병렬화 2 체제에서 <b>5x/b> 배 가속을 얻었으며 TP=1 모드에서 FasterTransformer를 사용하여 3.5배 가속도를 얻습니다. 훌륭한 성능을 확인할 수 있었습니다! 이번 C++ 예제와 PyTorch/TF 바인딩을 활용하여 복잡한 추론 파이프라인에서 적용할 수 있습니다. </br> \n","동시에, 보다 통합된 추론 엔진/서빙 솔루션을 원하신다면 다음 노트북에서 살펴보도록 하겠습니다."]},{"cell_type":"markdown","metadata":{"id":"6hwmRatDczXN"},"source":["<h2 style=\"color:green;\">축하합니다!</h2>\n","\n","[Triton Inference Server와 FasterTransformer를 백엔드로 하는 GPT-J 6b 모델의 추론](04_FTAndTritonRunRemoteInferenceOfTheGPT-J.ipynb)으로 이동하십시오."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"03_FTRunInferenceOfTheGPT-J.ipynb","provenance":[],"collapsed_sections":["k8EpO8DBczW8"]}},"nbformat":4,"nbformat_minor":0}